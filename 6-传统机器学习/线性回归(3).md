基本概念：

线性回归（Linear regression）是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。

线性回归的应用范围很广，可以应用在包括金融，趋势线，经济学，传染病学等一系列常见领域，是一种很常见的也是最为简单的一种模型。大多情况下的应用均为预测或者映射，线性回归可以用来对观测数据集的和X的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的X值，在没有给定与它相配对的y的情况下，可以用这个拟合过的模型预测出一个y值。接下来本章的讨论也均为建立在这种问题之上。

## 1.1一元线性回归
### 1.1.1函数模型
一元线性回归线性回归，又称简单线性回归（simple linear regression, SLR），是最简单但用途很广的回归模型。

其函数模型为：
Y=AX+B
我们用一元线性模型来拟合目前所拥有的数据，来预测未知的数据。所谓拟合就是用一个函数模型去逼近实际的数据分布。这里涉及到怎样确定函数模型才能使拟合程度更高（预测值与实际值偏差最小）的问题，我们一般使用的是最小二乘法。

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/EF7B9A037DF44CDD91092B31A0C89DD8/12)
图一

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/45B9570E9F6D4F6690C1157794C32001/13)
图二

如图所示，假设为房屋面积与售价的关系，我们针对这种图一所示分布的数据，可以采用一条直线来拟合模型的整体走势。具体解法见下一节最小二乘法内容。

### 1.1.2最小二乘法
确定我们使用一元线性模型来解决这个问题。建立一个模型之后，想知道的第一个问题就是。这个模型的准确度怎么样。那衡量的标准是什么呢，就是保证所有数据偏差的平方和最小。这就引入了我们本届要讲的最小二乘法。最小二乘法使在线性回归下采用最小二乘准则（或者说叫做最小平方），进行线性拟合参数求解的、矩阵形式的公式方法。这种方法求得是全局最优解。
给出一组{Xi，Yi}的数据，我们首先可以绘制出有关数据的散点图
（使用数据集 http://archive.ics.uci.edu/ml/datasets/Housing 只取部分面积房价数据）

Python代码如下：

```
# -*- coding: utf-8 -*
import numpy as np
import os
import matplotlib.pyplot as plt
def drawDiagram(fileName):
    #改变工作路径到数据文件存放的地方
    os.chdir("d:/data")
    xcord=[];ycord=[]
    fr=open(fileName)
    for line in fr.readlines():
        lineArr=line.strip().split()
        xcord.append(float(lineArr[1]));ycord.append(float(lineArr[2]))
    plt.scatter(xcord,ycord,s=30,c='red',marker='s')
    plt.show()

```
我们任取两点建立一个模型Y=Ax+B，那接下来就进入最开始的问题了，我们要看这个A，B对应的模型是不是最优模型，即是不是能让所有数据偏差的平方和最小。
即![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/4EA3A58577A44F2E98112F0340A240AE/27)我们现在要求使M最小的a，b的值。方程为a，b为自变量，M为因变量的函数，可对M求关于a，b偏导求得。
求解下面方程组可以求得
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/167C0182AEB546F98668C9EEB77E27B4/26)

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/46682085886D47F487CB06E7F0A90CCB/28)

Python实现代码如下：

```
# -*- coding: utf-8 -*
import numpy as np
import os
import matplotlib.pyplot as plt
def drawDiagram(fileName):
    #改变工作路径到数据文件存放的地方
    os.chdir("d:/workspace_ml")
    xcord=[];ycord=[]
    fr=open(fileName)
    for line in fr.readlines():
        lineArr=line.strip().split()
        xcord.append(float(lineArr[1]));ycord.append(float(lineArr[2]))
    plt.scatter(xcord,ycord,s=30,c='red',marker='s')
    #a=0.1965;b=-14.486
    a=0.1612;b=-8.6394
    x=np.arange(90.0,250.0,0.1)
    y=a*x+b
    plt.plot(x,y)
    plt.show()

```
## 1.2多元线性回归
讲完一元线性回归后，我们把范围稍微拓展一下，由单个自变量变为多个自变量，这称之为多元线性回归。还是以房价预测为例子，上一节我们只取了面积与房价的关系，实际来说情况要复杂得多，可能还会与房间数，卧室数目等一系列别的问题相关。
设p个自变量（X1,X2…Xp）m个因变量，有总体回归模型如下
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/F027AC01E70E4092AB56EB47012C6219/47)
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/21ED211D03DD466FB202B944AD587C38/46)

......

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/4B4B9E83322949CDAC9B80F6EED88B54/45)

也就是假设每个指标Yi与自变量Xj之间存在线性关系。直观来说，还是以之前的房价数据集为例子，我们可以认为房间数，卧室数等不同的因素占的权重不同。100平的一居室价格可能低于100平的二居室。误差项e=[e1,e2,…,em]’满足假设
Ee=0,Cov(e)=∑▒=(σij)
上面的模型可以划为相应的矩阵形式：

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/F9EB4AFB9B254942986EA7504DDC9BC0/43)

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/DA2E02963024400EB539F4EF13F56848/44)

对任意i，Xi0=1

参数矩阵与随机测量误差为

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/096798648BE64914B37CDC5FB17BA09C/42)

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/31761AE058084572803BC2F04EE65365/41)

多元线性回归模型的表述为

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/DF713E69787B4A08B260FB0B9DC160D1/48)

针对这种多元线性回归，明显已经很难一步步实现整个算法模型，我们推荐使用从sklearn等框架中直接调用线性回归模型。数据集依然使用经典的波士顿房价预测问题。
Python代码如下：

```
import matplotlib.pylab as plt
import numpy as np
from sklearn import linear_model

datasets_x = []
datasets_y = []

fr = open('prices.txt', 'r')

lines = fr.readlines()

for line in lines:
    items = line.strip().split(',')
    datasets_x.append(int(items[0]))
    datasets_y.append(int(items[1]))

print(datasets_x,'---------------------------------', datasets_y)

length = len(datasets_x)
datasets_x = np.array(datasets_x).reshape([length, 1])
datasets_y = np.array(datasets_y)

print(datasets_x)
print(datasets_y)

minX = min(datasets_x)
maxX = max(datasets_x)

print(minX)
print(maxX)

x = np.arange(minX, maxX)

print(x)

x = np.arange(minX, maxX).reshape([-1, 1])
print(x)

linear = linear_model.LinearRegression()
linear.fit(datasets_x, datasets_y)

#
plt.scatter(datasets_x, datasets_y, color = 'red')

plt.plot(x, linear.predict(x), color = 'blue')
plt.xlabel('Area')
plt.ylabel('Price')

```
## 1.3广义线性回归
在这之前我们已经将由单一自变量拓宽到了矩阵，接下来我们进一步拓宽应用范围，将范围拓宽到G（y）=Ax+b。即将y变为关于y的函数，G（y）可以为任意曲线方程。通过非线性关联函数，总体的均值因线性预测变量而异。响应概率分布可以是指数分布系列中的任意一种。

这给我们之后的处理带了很大的空间。在这里仅做简单介绍。

## 1.4梯度下降
在上面我们介绍了最小二乘法来进行计算线性回归，当我们进行下一步，拓展到非线性的时候，不满足基本的线性假设此时最小二乘法就不适应了。我们接下来引入梯度下降法来解决这个问题。
###  1.4.1梯度
首先我们要知道的是什么是梯度。
梯度是一个向量，方向指向标量场增加最快的方向（通俗的说就是从山顶下山最陡峭的方向），大小为长度为一得方向中函数最大的增加率。在微积分里，梯度就是对函数的各个自变量求偏导，然后以向量的形式写出来就是梯度。
### 1.4.2梯度下降
从几何意义上来讲，梯度指向变化率最快的方向，那么沿着梯度一直走我们就能找到最大值，或者沿着相反方向我们就能找到最小值。但这里出现一个问题，就是我们选择的下降方向只是在当前最优解，最后到达的也可能不是全局最优解，而只是一个局部最优解。直观地讲，我们从山顶往山下走。每一次（每走一步停下来）都选择最陡峭的地方（梯度方向）往下走，如过是一个单独山峰的话，我们很快就能到达最低点，也就是山脚（如下图一）。但如果是一系列的山峰相连的山地也有可能我们走到某个山峰中间的凹地（如下图二），而不是最终想到达的山谷就停下来了。从这儿我们可以看出有两个数据是至关重要的，就是我们出发的初始位置，还有我们每次从山峰往下前行的距离。这就需要我们对算法参数进行一些调整。
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/0DF873AEEF27416EA5B14C8E91C66B4D/159)
 图一
 ![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/F16EC44C0A6042FE9B2552FD143ED292/150)
 图二
 
我们回到最开始的房价预测问题中，我们利用一系列的分量来预测房子的最终售价。比如x1=房间的面积，x2=房间的朝向，x3=卧室的个数等。据此我们可以做出一个估计函数。

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/9B938FB2E928449F8748CF87E39E7503/155)
 
θ在这儿称为参数，意思为这个xN在我们整个衡量函数中包含的比重。如果我们令X0 = 1，就可以用向量的方式来表示了：

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/6299744C7DEF414BBFE641B13A32A238/156)
 
接下来出现了一个新的问题，我们怎样评估我们假设得到的模型与实际的接近程度呢？为了能够量化这个差距，我们引入损失函数来进行评估。

 ![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/D7B885C89A484157ACEE780CA5CF0E1B/154)
 
这个错误估计函数是去对x(i)的估计值与真实值y(i)差的平方和作为错误估计函数（前面乘上的1/2是为了在求导的时候，约去这个系数）。是损失函数最小的方法，我们之前已经描述了一种方法为最小二乘法。最小二乘法主要针对的是线性模型，最优解的情况。我们现在介绍的梯度下降法是一种迭代的方法，即可用于求解最小二乘问题，又可用于求解其他更多种类的问题。
梯度下降的流程如下：
    1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。
2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。
我们回到上面的图二中去，图二表示的就是参数θ与误差函数J(θ)的关系图，红色部分表示J(θ)取值较高，我们需要尽量往低处也就是深蓝色部分走。最后下降到无法下降为止。
使用数学语言来描述的话，过程如下：
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/E61858A1FC424BE689680ED284F2A8C6/151)
 
之后我们进行更新，θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长（可以理解为下山的步幅）。

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/0F2317B27A8347D2BDCCF2DFB1178A2B/152)
 
对于向量，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。
简单来说就是下面这样的迭代运算。
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/A8AB8304B3084ADAA7C32B7F5A4FFF31/153)
 

### 1.4.3 梯度下降的算法调优
1. 算法的步长选择。
实际上算法取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。
2. 算法参数的初始值选择。
初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。
3归一化。
不同特征的取值范围不一样，这回事迭代变得十分缓慢，我们为了减少特征，可以对特征数据归一化。这会极快的加大迭代速度。
1.4.4梯度下降的三种方式
1批量梯度下降
具体做法就是说，同时对所有样本进行梯度迭代。这是最常用的计算方式。需求运算量比较大。
2随机梯度下降法
这与方法一刚好处于两个极端。一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。
3小批量梯度下降
这种方法兼具上面两种方法的优点。基本原理与批量下降相同，不同的是针对的是使用第一种方法是需求的运算量太大，无法运行时，我们从中选取一部分进行运行。




