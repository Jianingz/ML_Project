![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/EF7B9A037DF44CDD91092B31A0C89DD8/12)
图一

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/45B9570E9F6D4F6690C1157794C32001/13)
图二

如图所示，假设为房屋面积与售价的关系，我们针对这种图一所示分布的数据，可以采用一条直线来拟合模型的整体走势。具体解法见下一节最小二乘法内容。

### 1.1.2最小二乘法
确定我们使用一元线性模型来解决这个问题。建立一个模型之后，想知道的第一个问题就是。这个模型的准确度怎么样。那衡量的标准是什么呢，就是保证所有数据偏差的平方和最小。这就引入了我们本届要讲的最小二乘法。最小二乘法使在线性回归下采用最小二乘准则（或者说叫做最小平方），进行线性拟合参数求解的、矩阵形式的公式方法。这种方法求得是全局最优解。
给出一组{Xi，Yi}的数据，我们首先可以绘制出有关数据的散点图
（使用数据集 http://archive.ics.uci.edu/ml/datasets/Housing 只取部分面积房价数据）

Python代码如下：

```
# -*- coding: utf-8 -*
import numpy as np
import os
import matplotlib.pyplot as plt
def drawDiagram(fileName):
    #改变工作路径到数据文件存放的地方
    os.chdir("d:/data")
    xcord=[];ycord=[]
    fr=open(fileName)
    for line in fr.readlines():
        lineArr=line.strip().split()
        xcord.append(float(lineArr[1]));ycord.append(float(lineArr[2]))
    plt.scatter(xcord,ycord,s=30,c='red',marker='s')
    plt.show()

```
我们任取两点建立一个模型Y=Ax+B，那接下来就进入最开始的问题了，我们要看这个A，B对应的模型是不是最优模型，即是不是能让所有数据偏差的平方和最小。
即![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/4EA3A58577A44F2E98112F0340A240AE/27)我们现在要求使M最小的a，b的值。方程为a，b为自变量，M为因变量的函数，可对M求关于a，b偏导求得。
求解下面方程组可以求得
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/167C0182AEB546F98668C9EEB77E27B4/26)

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/46682085886D47F487CB06E7F0A90CCB/28)

Python实现代码如下：

```
# -*- coding: utf-8 -*
import numpy as np
import os
import matplotlib.pyplot as plt
def drawDiagram(fileName):
    #改变工作路径到数据文件存放的地方
    os.chdir("d:/workspace_ml")
    xcord=[];ycord=[]
    fr=open(fileName)
    for line in fr.readlines():
        lineArr=line.strip().split()
        xcord.append(float(lineArr[1]));ycord.append(float(lineArr[2]))
    plt.scatter(xcord,ycord,s=30,c='red',marker='s')
    #a=0.1965;b=-14.486
    a=0.1612;b=-8.6394
    x=np.arange(90.0,250.0,0.1)
    y=a*x+b
    plt.plot(x,y)
    plt.show()

```
## 1.2多元线性回归
讲完一元线性回归后，我们把范围稍微拓展一下，由单个自变量变为多个自变量，这称之为多元线性回归。还是以房价预测为例子，上一节我们只取了面积与房价的关系，实际来说情况要复杂得多，可能还会与房间数，卧室数目等一系列别的问题相关。
设p个自变量（X1,X2…Xp）m个因变量，有总体回归模型如下
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/F027AC01E70E4092AB56EB47012C6219/47)
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/21ED211D03DD466FB202B944AD587C38/46)

......

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/4B4B9E83322949CDAC9B80F6EED88B54/45)

也就是假设每个指标Yi与自变量Xj之间存在线性关系。直观来说，还是以之前的房价数据集为例子，我们可以认为房间数，卧室数等不同的因素占的权重不同。100平的一居室价格可能低于100平的二居室。误差项e=[e1,e2,…,em]’满足假设
Ee=0,Cov(e)=∑▒=(σij)
上面的模型可以划为相应的矩阵形式：

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/F9EB4AFB9B254942986EA7504DDC9BC0/43)

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/DA2E02963024400EB539F4EF13F56848/44)

对任意i，Xi0=1

参数矩阵与随机测量误差为

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/096798648BE64914B37CDC5FB17BA09C/42)

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/31761AE058084572803BC2F04EE65365/41)

多元线性回归模型的表述为

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/DF713E69787B4A08B260FB0B9DC160D1/48)

针对这种多元线性回归，明显已经很难一步步实现整个算法模型，我们推荐使用从sklearn等框架中直接调用线性回归模型。数据集依然使用经典的波士顿房价预测问题。
Python代码如下：

```
import matplotlib.pylab as plt
import numpy as np
from sklearn import linear_model

datasets_x = []
datasets_y = []

fr = open('prices.txt', 'r')

lines = fr.readlines()

for line in lines:
    items = line.strip().split(',')
    datasets_x.append(int(items[0]))
    datasets_y.append(int(items[1]))

print(datasets_x,'---------------------------------', datasets_y)

length = len(datasets_x)
datasets_x = np.array(datasets_x).reshape([length, 1])
datasets_y = np.array(datasets_y)

print(datasets_x)
print(datasets_y)

minX = min(datasets_x)
maxX = max(datasets_x)

print(minX)
print(maxX)

x = np.arange(minX, maxX)

print(x)

x = np.arange(minX, maxX).reshape([-1, 1])
print(x)

linear = linear_model.LinearRegression()
linear.fit(datasets_x, datasets_y)

#
plt.scatter(datasets_x, datasets_y, color = 'red')

plt.plot(x, linear.predict(x), color = 'blue')
plt.xlabel('Area')
plt.ylabel('Price')

```
## 1.3广义线性回归
在这之前我们已经将由单一自变量拓宽到了矩阵，接下来我们进一步拓宽应用范围，将范围拓宽到G（y）=Ax+b。即将y变为关于y的函数，G（y）可以为任意曲线方程。通过非线性关联函数，总体的均值因线性预测变量而异。响应概率分布可以是指数分布系列中的任意一种。

这给我们之后的处理带了很大的空间。在这里仅做简单介绍。

## 1.4梯度下降
在上面我们介绍了最小二乘法来进行计算线性回归，当我们进行下一步，拓展到非线性的时候，不满足基本的线性假设此时最小二乘法就不适应了。我们接下来引入梯度下降法来解决这个问题。


