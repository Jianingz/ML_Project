# 2.1 基础知识
## 2.1.1二类回归问题


回归分析是一种统计学分析数据的方法，目的在于了解两个或多个变数间是否相关、相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。回归在数学上来说就是给定一个点集合，能够用一条曲线去拟合。根据曲线的差异分为线性回归，二次回归，Logistic回归等。

首先逻辑回归这一模型，主要应用于二类分类问题上（也可用于多类分类）。即区分两种对立的类别。它是直接对分类的可能性进行建模的，无需事先假设数据分布，这样就避免了假设分布不准确所带来的问题，因为它是针对于分类的可能性进行建模的，所以它不仅能预测出类别，还可以得到属于该类别的概率。

## 2.1.2单位阶跃到sigmoid函数

针对二类回归问题建模，我们首先能想到的就是，如图一所示的单位阶跃函数。
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICEB762A8DF86604DB88E8D33688415F139/187)图一

但阶跃函数存在跳变点，无法在工程上实现，我们要做的就是找到最适合工程应用的，最接近这一函数的相关函数来取代阶跃函数进行建模。
那么最接近的函数，我们一般选取的就是逻辑斯蒂函数（logistics），如图二所示。
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE12A53E1C4CA34FE69CC450D3EAB1ADCF/188)图二

sigmoid函数是如下所示
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE8D290E96BB574F51B41230DAE1D9F462/189)

## 2.1.3回归

回归分析是一种统计学分析数据的方法，目的在于了解两个或多个变数间是否相关、相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。回归在数学上来说就是给定一个点集合，能够用一条曲线去拟合。根据曲线的差异分为线性回归，二次回归，Logistic回归等。

# 2.2最大似然估计
最大似然估计是用来估计一个概率模型的参数的一种方法。
## 2.2.1贝叶斯决策
通常，事件A在事件B（发生）的条件下的概率，与事件B在事件A（发生）的条件下的概率是不一样的。然而，这两者是有确定的关系的，贝叶斯定理就是这种关系的陈述。贝叶斯公式的一个用途在于通过已知的三个概率函数推出第四个。公式如下：
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICECAC565BD31754517B18E37BF3B2D2218/201)

其中P(A|B)是指在事件B发生的情况下事件A发生的概率。

在贝叶斯定理中，每个名词都有约定俗成的名称：

P(A|B)是已知B发生后A的条件概率，也由于得自B的取值而被称作A的后验概率。

P(A)是A的先验概率（或边缘概率）。之所以称为"先验"是因为它不考虑任何B方面的因素。

P(B|A)是已知A发生后B的条件概率，也由于得自A的取值而被称作B的后验概率。

P(B)是B的先验概率或边缘概率。

按这些术语，贝叶斯定理可表述为：

后验概率 = (似然性*先验概率)/标准化常量

也就是说，后验概率与先验概率和相似度的乘积成正比。
另外，比例P(B|A)/P(B)也有时被称作标准似然度（standardised likelihood），贝叶斯定理可表述为：

后验概率 = 标准似然度*先验概率

## 2.2.2最大似然估计
在实际问题中，我们经常遇到获得的数据只是有限数目的样本数据，而先验概率（某一不确定量p的先验概率分布是在考虑"观测数据"前，能表达p不确定性的概率分布。它旨在描述这个不确定量的不确定程度，而不是这个不确定量的随机性。这个不确定量可以是一个参数，或者是一个隐含变量）与类条件概率（类条件概率密度是，假定x是一个连续随机变量，其分布取决于类别状态，表示成p(x|ω)的形式）都是未知的。比如我们要获取人群中关于吸烟人数的死亡比率的数据，往往只能获取死亡的人中的吸烟烟民的比例，而并不能直接获得人群中吸烟的比例。根据现有的样本数据进行分类时一种可行的方法是需要先对先验概率与类条件概率进行估计，然后再套用贝叶斯分类器。

先验概率的估计较简单，1、每个样本所属的自然状态都是已知的（有监督学习）；2、依靠经验；3、用训练样本中各类出现的频率估计。

类条件概率的估计（非常难），原因包括：概率密度函数包含了一个随机变量的全部信息；样本数据可能不多；特征向量x的维度可能很大等等。总之要直接估计类条件概率的密度函数很难。解决的办法就是，把估计完全未知的概率密度转化为估计参数。这里就将概率密度估计问题转化为参数估计问题，极大似然估计就是一种参数估计方法。当然了，概率密度函数的选取很重要，模型正确，在样本区域无穷时，我们会得到较准确的估计值，如果模型都错了，那估计半天的参数，肯定也没啥意义了。

上面说到，参数估计问题只是实际问题求解过程中的一种简化方法（由于直接估计类条件概率密度函数很困难）。所以能够使用极大似然估计方法的样本必须需要满足一些前提假设。重要前提：训练样本的分布能代表样本的真实分布。每个样本集中的样本都是所谓独立同分布的随机变量，且有充分的训练样本。

总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。

原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

## 2.3梯度下降
在上面我们介绍了最小二乘法来进行计算线性回归，当我们进行下一步，拓展到非线性的时候，不满足基本的线性假设此时最小二乘法就不适应了。我们接下来引入梯度下降法来解决这个问题。
###  2.3.1梯度
首先我们要知道的是什么是梯度。
梯度是一个向量，方向指向标量场增加最快的方向（通俗的说就是从山顶下山最陡峭的方向），大小为长度为一得方向中函数最大的增加率。在微积分里，梯度就是对函数的各个自变量求偏导，然后以向量的形式写出来就是梯度。
### 2.3.2梯度下降
从几何意义上来讲，梯度指向变化率最快的方向，那么沿着梯度一直走我们就能找到最大值，或者沿着相反方向我们就能找到最小值。但这里出现一个问题，就是我们选择的下降方向只是在当前最优解，最后到达的也可能不是全局最优解，而只是一个局部最优解。直观地讲，我们从山顶往山下走。每一次（每走一步停下来）都选择最陡峭的地方（梯度方向）往下走，如过是一个单独山峰的话，我们很快就能到达最低点，也就是山脚（如下图一）。但如果是一系列的山峰相连的山地也有可能我们走到某个山峰中间的凹地（如下图二），而不是最终想到达的山谷就停下来了。从这儿我们可以看出有两个数据是至关重要的，就是我们出发的初始位置，还有我们每次从山峰往下前行的距离。这就需要我们对算法参数进行一些调整。

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/0DF873AEEF27416EA5B14C8E91C66B4D/159)
 图一
 
 ![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/F16EC44C0A6042FE9B2552FD143ED292/150)
 图二
 
我们回到最开始的房价预测问题中，我们利用一系列的分量来预测房子的最终售价。比如x1=房间的面积，x2=房间的朝向，x3=卧室的个数等。据此我们可以做出一个估计函数。

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/9B938FB2E928449F8748CF87E39E7503/155)
 
θ在这儿称为参数，意思为这个xN在我们整个衡量函数中包含的比重。如果我们令X0 = 1，就可以用向量的方式来表示了：

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/6299744C7DEF414BBFE641B13A32A238/156)
 
接下来出现了一个新的问题，我们怎样评估我们假设得到的模型与实际的接近程度呢？为了能够量化这个差距，我们引入损失函数来进行评估。

 ![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/D7B885C89A484157ACEE780CA5CF0E1B/154)
 
这个错误估计函数是去对x(i)的估计值与真实值y(i)差的平方和作为错误估计函数（前面乘上的1/2是为了在求导的时候，约去这个系数）。是损失函数最小的方法，我们之前已经描述了一种方法为最小二乘法。最小二乘法主要针对的是线性模型，最优解的情况。我们现在介绍的梯度下降法是一种迭代的方法，即可用于求解最小二乘问题，又可用于求解其他更多种类的问题。
梯度下降的流程如下：
    1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。
2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。
我们回到上面的图二中去，图二表示的就是参数θ与误差函数J(θ)的关系图，红色部分表示J(θ)取值较高，我们需要尽量往低处也就是深蓝色部分走。最后下降到无法下降为止。
使用数学语言来描述的话，过程如下：
![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/E61858A1FC424BE689680ED284F2A8C6/151)
 
之后我们进行更新，θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长（可以理解为下山的步幅）。

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/0F2317B27A8347D2BDCCF2DFB1178A2B/152)
 
对于向量，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。
简单来说就是下面这样的迭代运算。

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/A8AB8304B3084ADAA7C32B7F5A4FFF31/153)
 

### 2.3.3 梯度下降的算法调优
1. 算法的步长选择。
实际上算法取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。
2. 算法参数的初始值选择。
初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。
3归一化。
不同特征的取值范围不一样，这回事迭代变得十分缓慢，我们为了减少特征，可以对特征数据归一化。这会极快的加大迭代速度。
### 2.3.4梯度下降的三种方式
#### 2.3.4.1批量梯度下降
具体做法就是说，同时对所有样本进行梯度迭代。这是最常用的计算方式。需求运算量比较大。
#### 2.3.4.2随机梯度下降法
这与方法一刚好处于两个极端。一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

代码实现

（1）随机梯度下降

上述算法，要进行maxCycles次循环，每次循环中矩阵会有m*n次乘法计算，所以时间复杂度（开销）是maxCycles*m*n，当数据量较大时，时间复杂度就会很大。因此，可以是用随机梯度下降法来进行算法改进。

```
def stocGradAscent(dataMatrix,classLabels):
    m,n=shape(dataMatrix)
    alpha=0.01
    weights=ones(n)
    for i in range(m):
        h=sigmoid(sum(dataMatrix[i] * weights))#数值计算
        error = classLabels[i]-h
        weights=weights + alpha * error * dataMatrix[i] #array 和list矩阵乘法不一样
    return weights
```
注意：gradAscent函数和这个stocGradAscent函数中的h和weights的计算形式不一样，因为 
前者是的矩阵的计算，类型是numpy的matrix，按照矩阵的运算规则进行计算。 
后者是数值计算，其类型是list，按照数值运算规则计算。

对随机梯度上升算法进行测试：

```
In [37]: dataMat,labelMat=logRegres.loadDataSet()
    ...: 
In [38]: weights=logRegres.stocGradAscent(array(dataMat),labelMat)
    ...: 
In [39]: logRegres.plotBestFit(mat(weights).transpose())
    ...:
```

（2）改进的随机梯度下降法

```
def stocGradAscent1(dataMatrix,classLabels,numIter=150):
    m,n=shape(dataMatrix)
    weights=ones(n)
    for j in range(numIter):
        dataIndex=list(range(m))
        for i in range(m):
            alpha=4/(1+i+j)+0.01#保证多次迭代后新数据仍然具有一定影响力
            randIndex=int(random.uniform(0,len(dataIndex)))#减少周期波动
            h=sigmoid(sum(dataMatrix[randIndex] * weights))
            error=classLabels[randIndex]-h
            weights=weights + alpha*dataMatrix[randIndex]*error
            del(dataIndex[randIndex])
    return weights
```

#### 2.3.4.3小批量梯度下降
这种方法兼具上面两种方法的优点。基本原理与批量下降相同，不同的是针对的是使用第一种方法是需求的运算量太大，无法运行时，我们从中选取一部分进行运行。

## 2.4梯度下降的优化算法

###  2.4.1牛顿法
在最优化的问题中，线性最优化至少可以使用单纯行法求解，但对于非线性优化问题，牛顿法提供了一种求解的办法。假设任务是优化一个目标函数f，求函数f的极大极小问题，可以转化为求解函数f的导数f’=0的问题，这样求可以把优化问题看成方程求解问题（f’=0）。

为了求解f’=0的根，把f（x）的泰勒展开，展开到2阶形式：
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICEE3546F5C2E3C484BA16B3107FFD6A6DE/202)

这个式子是成立的，当且仅当 Δx无线趋近于0。此时上式等价与：

![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICEEDB2D89AE0AB43E68F63E7AED94D5A60/203)

求解：

![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE3ED5FE9CB30549CB946E9F0D9B8098D8/204)

得出迭代公式：

![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE60D3026274EF4ACD88024317647F6C8C/205)

一般认为牛顿法可以利用到曲线本身的信息，比梯度下降法更容易收敛（迭代更少次数）

牛顿法算法如下：

输入：目标函数f(X)，梯度▽f(x),海赛矩阵H(x),精度要求ε；

输出：f(x)的极小点x*.

步骤一：取初始点x0,置k=0

步骤二：计算梯度▽f(x)

步骤三：||▽f(x)||〈ε，那么停止计算得到的x*=xk。

步骤四：计算H(x)

步骤五：![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE5BECC16B691D49ED89AAD70F6E00888E/207)

步骤六：转步骤二

特性：

1牛顿法收敛速度为二阶，对于正定二次函数一步迭代即达最优解。

2牛顿法是局部收敛的，当初始点选择不当时，往往导致不收敛

3牛顿法不是下降算法，当二阶海塞矩阵非正定时，不能保证产生方向是下降方向。

4二阶海塞矩阵必须可逆，否则算法进行困难。

5对函数要求苛刻（二阶连续可微，海塞矩阵可逆），而且运算量大。

代码实现

牛顿法逻辑回归实现

```
from numpy import *
from math import *
import operator
import matplotlib
import matplotlib.pyplot as plt
#logistic regression ＋ 牛顿方法
def file2matrix(filename1,filename2):#完成了文件读取、迭代运算及绘图
fr1 = open(filename1)#打开一个文件
arrayOflines1 = fr1.readlines()#返回一个行数组
numberOfLines1 = len(arrayOflines1)#计算行数
matrix = zeros((numberOfLines1,3))#生成一个全零二维数组，numberOflines1行 3列。其实数据只有两列，一列全是1.
row = 0
for line in arrayOflines1:
line = line.strip()#声明：s为字符串，rm为要删除的字符序列 s.strip(rm)删除s字符串中开头、结尾处，位于 rm删除序列的字符
#s.lstrip(rm) 删除s字符串中开头处，位于 rm删除序列的字符
#s.rstrip(rm) 删除s字符串中结尾处，位于 rm删除序列的字符
#注意：#1. 当rm为空时，默认删除空白符（包括'\n', '\r', '\t', ' ')
listFromLine = line.split(' ')#将一行按（）中的参数符号分开放入一个list中
listFromLine[0:0] = ['1']#在list中最前面插入1，上面说了有一列全为一
for index,item in enumerate(listFromLine):#将list中的字符串形式的，全转换为对应的数值型。
listFromLine[index] = eval(item)
matrix[row,:] = listFromLine[:]#每个list赋给对应的二维数组的对应行
row+=1
matrix = mat(matrix)#将数组转换为矩阵
fr1.close()
fr2 = open(filename2)
arrayOflines2 = fr2.readlines()
numberOfLines2 = len(arrayOflines2)
matrixy = zeros((numberOfLines2,1))
row = 0;
for line in arrayOflines2:
line = line.strip()
listFromLine = [line]
for index,item in enumerate(listFromLine):
listFromLine[index] = eval(item)
matrixy[row,:] = listFromLine[:]
row+=1
matrixy = mat(matrixy)
fr2.close()
tempxxt = dot(matrix.T,matrix).I#这一部分乘上下面的denominator()既为H.I（Hessian矩阵的逆）
theta=mat(zeros((3,1)))#初始θ参数，全零
for i in range(0,2000):#迭代2000次得到了比较好的结果。我采取的可能是全向量的形式计算，感觉迭代次数有点偏多
temphypo=Hypothesis(theta,matrix,row)
tempdenominator=denominator(temphypo,row)
tempnumerator=numerator(temphypo,matrixy,matrix)
theta = theta+dot(tempxxt,tempnumerator)/tempdenominator
temparray = ravel(Hypothesis(theta,matrix,row))
temptheta = ravel(theta)
for i in range(0,row):#根据hypothesis函数的值进行标记,方便绘图
if(temparray[i]>=0.5):
temparray[i]=1
else:
temparray[i]=0;
fig = plt.figure()#生成了一个图像窗口
ax = fig.add_subplot(111)#刚才窗口中的一个子图
ax.scatter(ravel(matrix[:,1]),ravel(matrix[:,2]),200,20*temparray)#生成离散点，参数分别为点的x坐标数组、y坐标数组、点的大小数组、点的颜色数组
x=linspace(-1,10,100)#起点为－1，终止10，100个元素的等差数组x
ax.plot(x,-(temptheta[0]+temptheta[1]*x)/temptheta[2])#绘制x为自变量的函数曲线
plt.show()
def Hypothesis(theta,x,row):#假设函数、是一个向量形式
hypo = zeros((row,1))
for i in range(0,row):
temp = exp(-dot(theta.T,x[i].T))
hypo[i,:] = [1/(1+temp)]
return hypo
def denominator(hypo,row):#分母部分
temp=zeros((row,1))
temp.fill(1)
temp=temp-hypo
temp=dot(hypo.T,temp)
return temp
def numerator(hypo,y,x):#牛顿方法的分子，我们要做的就是迭代使这一部分接近零
temp = y-hypo
temp = dot(temp.T,x)
return temp.T
```
### 2.4.2拟牛顿法
牛顿法的突出优点是收敛很快，但是运用牛顿法需要计算二阶偏导数，而且目标函数的Hessian矩阵可能非正定。为了克服牛顿法的缺点，人们提出了拟牛顿法，它的基本思想是用不包含二阶导数的矩阵近似牛顿法中的Hessian矩阵的逆矩阵。由于构造近似矩阵的方法不同，因而出现不同的拟牛顿法。

特征：

1只需用到函数的一阶梯度；（Newton法用到二阶Hessian阵）

2下降算法，故全局收敛；

3不需求矩阵逆；（计算量小）

4一般可达到超线性收敛；（速度快）

5有二次终结性。

拟牛顿法与原始牛顿法的区别在于增加了沿牛顿方向的一维搜索，其迭代公式为：

![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE37538FB136EA4EB8BCD2422B79DE9969/208)


其中![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICED97C7E07D5DB4649BD590D57CCA19604/209)为牛顿方向，

![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICEE94F6976F6E04EB3AE61311149069AFE/210)是由一维搜索的步长，也就是满足：

![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE99EAEEFD12E743DB9CC57709379EF7F7/211)
其实牛顿法就是阻尼牛顿法步长为1的特殊情况。

拟牛顿法算法：


输入：目标函数f(X)，梯度▽f(x),海赛矩阵H(x),精度要求ε；

输出：f(x)的极小点x*.

步骤一：取初始点x0,置k=0

步骤二：计算梯度▽f(x)

步骤三：||▽f(x)||〈ε，那么停止计算得到的x*=xk。

步骤四：计算H(x)

步骤五：从xk出发，沿着dk方向作一维搜索，![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE22D626F0D8E04641AE0CA9B38AA589BF/213)

步骤六：![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICEC56531DBA8E34D2595E625E6261B803E/214)

步骤七：转步骤二





# 2.5 优化算法比较
我们之前讲的好几种常见的无约束优化算法（包括最小二乘法，梯度下降法等），处理实际问题时各有不同的优劣。
## 2.5.1梯度下降法与最小二乘法
梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。
## 2.5.2梯度下降法与牛顿法
梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

针对具体的问题，我们可以选择不同的解法。
## 2.6逻辑回归模型
### 2.6.1逻辑回归模型
我们是基于前一章中的广义线性模型部分进行了推广，获得的逻辑回归模型。

逻辑回归是为了解决分类问题，根据一些已知的训练集训练好模型，再对新的数据进行预测属于哪个类。如图三所示，有一些属于两个类的数据，目标是判断圆圈属于哪一类。也就是说逻辑回归的目标是找到一个有足够好区分度的决策边界，从而能够将两类很好的分开。
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE2E8EC41C0241429BB0E3DFE854ECD152/190)图三

假设已经存在这样一个边界，针对于图中这种线性可分的情况，这条边界是输入特征向量的线性组合，假设输入的特征向量为x∈R^n (图中输入向量为二维)，YY取值为0，1。那么决策边界可以表示为w1x1+w2x2+b=0w1x1+w2x2+b=0，假如存在一个例子使得hw(x)=w1x1+w2x2+b>0hw(x)=w1x1+w2x2+b>0，那么可以判断它类别为1，这个过程实际上是感知机，即只通过决策函数的符号来判断属于哪一类。而逻辑斯蒂回归需要再进一步，它要找到分类概率P(Y=1)P(Y=1)与输入向量xx的直接关系，然后通过比较概率值来判断类别，而逻辑斯函数刚好可以满足这个要求。

我们利用广义线性回归模型的G（Y）=wx+b形式，将G（Y）替换为sigmoid函数，建立逻辑回归模型。

我们将sigmoid函数中的x自变量替换为wx+b，g（x）替换为y，则原公式变为：
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/0EABD990AECD45E19DC3F7139B36ABBF/237)

将此公式进行变形将关于自然常数的指数形式放到一边，y的函数放到另一边得到，
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/0EABD990AECD45E19DC3F7139B36ABBF/237)
两边分别取自然对数就可以将wx+b单独取出来，从而得到广义线性回归模型形式的逻辑斯蒂回归![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/0EABD990AECD45E19DC3F7139B36ABBF/237)
。
得到了输入向量x下导致产生两类的概率为:
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE47BC99901EDF4F318B64255FCF90A04F/191)

其中w称为权重，b称为偏置，其中的w⋅x+b看成对x的线性函数。然后对比上面两个概率值，概率值大的就是x对应的类。两项相加之和自然为1。分别对应不同分类的概率。

在逻辑回归中所定义的代价函数就是使得该件事情发生的几率最大，也就是某个样本属于其真实标记样本的概率越大越好。如，一个样本的特征x所对应的标记为1,通过逻辑回归模型之后，会给出该样本的标记为1和为-1的概率分别是多少，我们当然希望模型给出该样本属于1的概率越大越好，正因为我们的代价函数需要求的是最大值，所以后面会使用到梯度上升算法而不是梯度下降算法。为了求得记录的最大值，我们需要使用最大似然函数L，假定数据集中的每个样本都是相互独立的，L(w)的计算公式如下
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE03AFA740CD6E41C690E7986606F1B54C/192)
通过上面的公式可以发现，当似然函数的值非常小的时候还好，当数值比较大时指数函数会出现溢出等一系列问题，为了降低这种情况发生的可能性和方便对似然函数进行最大化处理，取似然函数的对数，变指数函数为和函数。
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE36CADBB941C04D65A601AD9CDDEBE3D5/193)
接下来得到了和函数形式的，我们进一步通过上一章中讲到的梯度下降法或者本章之后章节讲到的拟牛顿法对函数进行迭代求解，即可求得所需的逻辑斯蒂回归模型。
通过上面的公式可以发现，当y=0的时候，第一项为0，当y=1的时候第二项为0，损失函数如下
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE56EA7AB252C44B05980C46911F7F6538/194)
我们只需要修改激活函数和代价函数。在实现logistic回归之前，需要先计算出对数似然函数对于w的偏导，得到每次权重更新的Δω。
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE6C115CB24AE244D686996963F54E208C/195)
所以Δω应该为
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICEB90A04154796415399A835DA4EAC5814/196)

如果使用梯度下降算法，则ω=ω-Δω，如果使用梯度上升算法ω=ω+Δω。

简单来说就是使用固定的步长与似然函数的对数函数关于w微分的不断迭代，来寻找到最佳的函数模型。

分类边界

知道如何求解参数后，我们来看一下模型得到的最后结果是什么样的。很容易可以sigmoid函数看出，当θTx>0 时，y=1，否则 y=0y=0。θTx=0是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是，这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。

正则化

当模型的参数过多时，很容易遇到过拟合的问题。这时就需要有一种方法来控制模型的复杂度，典型的做法在优化目标中加入正则项，通过惩罚过大的参数来防止过拟合：
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE94C2A5C9AF074DCABA00DC1C4814606D/198)
一般情况下，取p=1p=1或p=2p=2，分别对应L1，L2正则化，两者的区别可以从下图中看出来，L1正则化（上面的图）倾向于使参数变为0，因此能产生稀疏解。

![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICE5B98303B70A14073B1219FCB0FCAAB9A/199)
![image](https://note.youdao.com/yws/public/resource/6e1b2896a8e7bc8da8fad452db15114a/xmlnote/OFFICEB58B1054792D4CDB8921D26287BCA128/200)

实际应用时，由于我们数据的维度可能非常高，L1正则化因为能产生稀疏解，使用的更为广泛一些。

### 2.6.2代码实现

梯度下降法求最佳回归系数
先定义函数来获取数去，然后定义分类函数sigmoid函数，最后利用梯度上升法求解回归系数w。

建立一个logRegres.py文件，输入如下代码：


```
from numpy import *
#构造函数来获取数据
def loadDataSet():
    dataMat=[];labelMat=[]
    fr=open('machinelearninginaction/Ch05/testSet.txt')
    for line in fr.readlines():
        lineArr=line.strip().split()
        dataMat.append([1.0,float(lineArr[0]),float(lineArr[1])])#特征数据集，添加1是构造常数项x0
        labelMat.append(int(lineArr[-1]))#分类数据集
    return dataMat,labelMat

def sigmoid(inX):
    return 1/(1+exp(-inX))

def gradAscent(dataMatIn,classLabels):
    dataMatrix=mat(dataMatIn) #(m,n)
    labelMat=mat(classLabels).transpose() #转置后(m,1)
    m,n=shape(dataMatrix)
    weights=ones((n,1)) #初始化回归系数，(n,1)
    alpha=0.001 #定义步长
    maxCycles=500 #定义最大循环次数
    for i in range(maxCycles):
        h=sigmoid(dataMatrix * weights) #sigmoid 函数
        error=labelMat - h #即y-h，（m,1）
        weights=weights + alpha * dataMatrix.transpose() * error #梯度上升法
    return weights
```

在Python命令符中输入代码对函数进行测试：


```
In [8]: import logRegres
   ...: 
In [9]: dataArr,labelMat=logRegres.loadDataSet()
   ...: 
In [10]: logRegres.gradAscent(dataArr,labelMat)
    ...: 
Out[10]: 
matrix([[ 4.12414349],
        [ 0.48007329],
        [-0.6168482 ]])
```

于是得到了回归系数。接下来根据回归系数画出决策边界

定义作图函数：


```
def plotBestFit(weights):
    import matplotlib.pyplot as plt
    dataMat,labelMat=loadDataSet()
    n=shape(dataMat)[0]
    xcord1=[];ycord1=[]
    xcord2=[];ycord2=[]
    for i in range(n):
        if labelMat[i]==1:
            xcord1.append(dataMat[i][1])
            ycord1.append(dataMat[i][2])
        else:
            xcord2.append(dataMat[i][1])
            ycord2.append(dataMat[i][2])
    fig=plt.figure()
    ax=fig.add_subplot(111)
    ax.scatter(xcord1,ycord1,s=30,c='red',marker='s')
    ax.scatter(xcord2,ycord2,s=30,c='green')
    x=arange(-3,3,0.1)
    y=(-weights[0,0]-weights[1,0]*x)/weights[2,0] #matix
    ax.plot(x,y)
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.show()
```

在Python的shell中对函数进行测试：


```
In [11]: weights=logRegres.gradAscent(dataArr,labelMat)

In [12]: logRegres.plotBestFit(weights)
    ...:
```

![image](https://note.youdao.com/yws/public/resource/f3907b75ffed04a0aae0a4ccbc564979/xmlnote/8C02F9F2310145CEA1445DF8BC5F455F/376)



