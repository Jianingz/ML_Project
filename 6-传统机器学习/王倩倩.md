# 6.3支持向量机

## 6.3.1模型简介

​        支持向量机，英文名称是support vector machine，所以人们在提及时使用的称呼通常是SVM。总的来说，SVM是一种有监督的二分类模型，如果要使用SVM做多类分类，需要利用多次二分类来实现。基本模型是将数据映射到多维空间中以点的形式存在，然后在特征空间上找到最佳的分离超平面使得训练集上正负样本间隔最大，学习策略就是间隔最大化，最终可转化为一个最终可转化为一个凸二次规划问题的求解。并且在引入核方法之后，SVM也可以用来解决非线性问题。

​        SVM模型的优点在于：

- 可以解决小样本的机器学习问题
- 相对于神经网络等算法没有局部极小值问题
- 可以处理高维数据集且效果较好
- 可以解决非线性问题
- 泛化能力强

​        SVM模型的缺点在于：

- 对缺失数据敏感
- 对于参数调节以及核函数的参数敏感
- 对于核函数的高纬映射解释能力不足
- 主要的应用领域在于文本分类、图像识别以及主要的二分类领域。



## 6.3.2SVM相关概念原理介绍

   上述描述中，有两个不太熟悉的概念，一个是支持向量，听起来比较数学化的名称，一个是间隔，下面我们来介绍这两个概念。
   在给定的训练样本集的情况下，我们想要把两种样本区分开，最容易想到也是最简单的方法就是在训练集的样本空间中找到一个划分超平面，能够将两种样本分隔开，但是能够达到这样效果的划分超平面不止一个甚至有非常多，有一些划分超平面虽然现在符合要求，但在样本集增加的情况下效果会变差，因此我们要做的就是寻找最优效果最好的那个划分超平面。
   直观来看，当然是位于两类样本集“正中间”的划分超平面是最合适的，这个划分超平面即能够把两种样本分隔开，又使得两种样本最靠近它的点距离直线最大
。而更靠近某一边的划分超平面对该边样本的变化反应会比较大，容易出现错误，“正中间”的划分超平面对训练样本局部变化的抗干扰能力更强。也就是说，“正中间”的划分超平面所产生的分类结果鲁棒性最佳对未出现过的数据泛化能力最强。
   在样本空间中，划分超平面可以用如下所示的线性方程来描述：
   **公式**
   ！[公式1]
其中

## 6.3.3核函数方法



## 6.3.4实例演示



# 6.4决策树

## 6.4.1模型简介

​        决策树模型是基于树理论实现的数据分类，很像数据结构中的B+树，是一种基本的分类和回归方法。决策树的模型是一颗树的形状结构，可以是二叉树也可以是多叉树。决策树中每个非结点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶结点存放一个类标签。使用决策树进行决策的过程就是从根结点开始，测试待分类项中相应的特征属性，并按照其值选择输出有向边，直到到达叶结点，也就是将实例分到叶结点的类中。决策树模型可以理解为对象属性和对象值之间的一种映射关系，学习得到的决策树既可以看作是多个 if-then 的规则，也可以看作是定义在特征空间与类空间上的条件概率分布。它对于噪声数据有很好的鲁棒性，并且能够从训练集中学习析取表达式。

​        决策树模型具有如下优点：

- 模型结构清晰，容易理解方便展示，并且可以提取出规则用作可视化分析；
- 标称型（有限的数据中取，如：是、否）和数值型（无限的具体的数据中取，如：1.23、4.56等）的数据可以同时处理；
- 对中间值缺失不敏感；
- 模型可以扩展到大型数据库中，且它的大小独立于数据库的大小。

​       决策树模型具有如下缺点：

- 容易出现过拟合的问题；
- 容易忽略数据本身属性之间的关联关系；
- ID3算法在计算信息增益时结果容易偏向数值比较多的特征。

​        决策树模型由于具有良好的分析能力、解释性等优点常被应用于企业管理实践，企业投资决策等决策应用中。

   决策树的核心树的分裂。到底该选择什么来决定树的分叉是决策树构建的基础。最好的方法是利用信息熵实现。熵这个概念很头疼，很容易让人迷糊，简单来说就是信息的复杂程度。信息越多，熵越高。所以决策树的核心是通过计算信息熵划分数据集。



## 6.4.2决策树相关概念





## 6.4.3常用决策树算法

**ID3算法**

​**C4.5算法** 

**CART算法**

​    



## 6.4.4决策树算法流程

**ID3算法流程**

输入:训练数据集 D，特征集 A，阈值; 

输出:决策树 T

**C4.5算法流程**

输入:训练数据集 D，特征集 A，阈值; 

输出:决策树 T 

**CART算法流程**

输入:训练数据集 D，停止计算的条件; 



## 6.4.5实例演示

