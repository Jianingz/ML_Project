# 6.3支持向量机

## 6.3.1模型简介

​         支持向量机，英文名称是support vector machine，人们在提及时使用的称呼常用SVM。原始的支持向量机算法是由由弗拉基米尔·万普尼克和亚历克塞·泽范兰杰斯于1963年提出的，直至1992年，Bernhard E. Boser、Isabelle M. Guyon和弗拉基米尔·万普尼克提出将核技巧应用于此，使得支持向量机模型应用更广，而当前标准的前身（软间隔）是由Corinna Cortes和Vapnik于1993年提出的。

​	总的来说，支持向量机是一种有监督的二分类模型，如果要使用支持向量机做多类分类，需要利用多次二分类来实现。支持向量机的基本模型是将数据映射到多维空间中以点的形式存在，然后在特征空间上找到最佳的分离超平面使得训练集上正负样本间隔最大，学习策略就是间隔最大化，分类问题最终可转化为一个凸二次规划问题的求解。并且在引入核方法之后，支持向量机还可以用于解决非线性问题。

​        支持向量机模型的优点在于：

- 可以解决小样本的机器学习问题
- 相对于神经网络等算法没有局部极小值问题
- 可以处理高维数据集且效果较好
- 可以解决非线性问题
- 泛化能力强

​        支持向量机模型的缺点在于：

- 对缺失数据敏感

- 对于参数调节以及核函数的参数敏感

- 对于核函数的高纬映射解释能力不足

- 主要的应用领域在于文本分类、图像识别以及主要的二分类领域。

   支持向量机常被用于以下情景：

- 文本和超文本的分类，在归纳和直推方法中都可以明显减少所需要的有类标的样本数。

- 图像分类，有实验表明，在三四轮相关反馈之后支持向量机的搜索准确度明显高于其他传统的查询优化方案。

- 手写字体识别

- 医学中应用于蛋白质分类，有实验表明化合物的分类正确率高达90%以上，在生物科学中具有重大意义。



## 6.3.2SVM相关概念原理介绍

​	上述描述中，有两个不太熟悉的概念，一个是支持向量，听起来比较数学向的名称，一个是间隔，下面我们来介绍这两个概念。

​	在给定的训练样本集的情况下，我们想要把两种样本区分开，最容易想到也是最简单的方法就是在训练集的样本空间中找到一个划分超平面，能够将两种样本分隔开，但是能够达到这样效果的划分超平面不止一个甚至有非常多，有一些划分超平面虽然现在符合要求，但在样本集增加的情况下效果会变差，因此我们要做的就是寻找最优效果最好的那个划分超平面。

​	直观来看，当然是位于两类样本集“正中间”的划分超平面是最合适的，这个划分超平面即能够把两种样本分隔开，又使得两种样本最靠近它的点距离直线最大。而更靠近某一边的划分超平面对该边样本的变化反应会比较大，容易出现错误，“正中间”的划分超平面对训练样本局部变化的抗干扰能力更强。也就是说，“正中间”的划分超平面所产生的分类结果鲁棒性最佳对未出现过的数据泛化能力最强。

​	在样本空间中，设 ![n](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) 点测试集：![{\displaystyle ({\vec {x}}_{1},y_{1}),\,\ldots ,\,({\vec {x}}_{n},y_{n})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/1c25d448b128d21a677a09f5a002534dc702541c)，其中 ![y_{i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/67d30d30b6c2dbe4d6f150d699de040937ecc95f) 是 1 或者 −1，表明点 ![{\displaystyle {\vec {x}}_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/97dc64e456d28b08c449ec343111cc5c3ce39f72) 所属的类，![{\displaystyle {\vec {x}}_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/97dc64e456d28b08c449ec343111cc5c3ce39f72) 中每个都是一个![p](https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36) 维实向量。划分超平面可以用如下所示的线性方程来描述：

![{\displaystyle {\vec {w}}\cdot {\vec {x}}-b=0,\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/10d84705d651f94bf400037b21ccaa2568384244)

其中 ![{\displaystyle {\vec {w}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b6c48cdaecf8d81481ea21b1d0c046bf34b68ec)是该法向量。参数![{\displaystyle {\tfrac {b}{\|{\vec {w}}\|}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b4203b6e269e720d13207a93a931418fc6dac9f0) 决定从原点沿法向量![{\displaystyle {\vec {w}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b6c48cdaecf8d81481ea21b1d0c046bf34b68ec) 到超平面的偏移量。

​	如果这些样本数据是线性可分的，就可以选择两个平行超平面来分离正负数据集，使正负数据集的距离尽可能的大，这两个超平面可以由以下方程组表示：![{\displaystyle {\vec {w}}\cdot {\vec {x}}-b=1\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a677ed33ca0c840aa9295405fc095c8aefa73e48)或是![{\displaystyle {\vec {w}}\cdot {\vec {x}}-b=-1.\,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/6c3dbeeb7d5af27a2551ec07ce172ddbce62fc58)这两个超平面之间的区域就被称为“**间隔**”，划分超平面即为位于这两个平面正中间的超平面。通过几何可得两个超平面之间的距离为![{\displaystyle {\tfrac {2}{\|{\vec {w}}\|}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac553c94aec996dffa3369adcf285319178a474f)，因此为让两个超平面之间的距离最大，我们需要将 ![{\displaystyle \|{\vec {w}}\|}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5b6f27a892f3053ef0bfe273f88f18351a1a18ac)最小化，同时我们需要一些约束条件来使每个样本数据都位于间隔的正确一侧，即

![{\displaystyle {\vec {w}}\cdot {\vec {x}}_{i}-b\geq 1,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e07d4690468e1bbc2d9d9ac9ecfc63827072d6bd) 若![{\displaystyle y_{i}=1}](https://wikimedia.org/api/rest_v1/media/math/render/svg/13864bfc60110b9306a50b300d2d72b22fa1e371)

 或

![{\displaystyle {\vec {w}}\cdot {\vec {x}}_{i}-b\leq -1,}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c4e8211ca89d01512ac52d87f325d732227a04e3) 若![{\displaystyle y_{i}=-1.}](https://wikimedia.org/api/rest_v1/media/math/render/svg/54b778308143c47d354eba0f7b7299a0a939bde2)

​	由上述描述可知，划分超平面其实是由最靠近它的那些![{\displaystyle {\vec {x}}_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/97dc64e456d28b08c449ec343111cc5c3ce39f72) 确定的，而这些 ![{\displaystyle {\vec {x}}_{i}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/97dc64e456d28b08c449ec343111cc5c3ce39f72) 就叫做“**支持向量**”。   

## 6.3.3核函数方法

​	以上针对的是线性可分的情况下，如果遇到线性不可分的情况应该怎么办呢？幸运的是，如果原始空间是有限维的，即属性数有限，那么就一定存在一个高维空间能够使样本可分。

​	而引出这种方法前要先介绍一下核函数，核函数早在1964年的时候就被Aizermann等在势函数方法的研究中引入到机器学习领域中，但是直到1992年的时候才被Vapnik等成功应用到将线性SVMs推广到非线性SVMs，从此核函数的潜力才得以充分挖掘，核函数的主要作用是将数据从低维空间映射到高维空间。

​	首先来看定义：支持向量机通过某非线性变换 φ( x) ，将输入空间映射到高维特征空间。特征空间的维数可能非常高。如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数 K(x, x′) ，它恰好等于在高维空间中这个内积，即K( x, x′) =<φ( x) ⋅φ( x′) > 。那么支持向量机就不用计算复杂的非线性变换，而由这个函数 K(x, x′) 直接得到非线性变换的内积，使大大简化了计算。这样的函数 K(x, x′) 称为核函数。

​	核函数将原始数据的维度变换，使得原样本空间线性不可分的样本点在变维之后的空间线性可分，这样可以很好的解决数据的非线性问题，而无需考虑映射过程。具体了解此过程凸优化理论知识，支持向量机即是在用对偶理论来求解一个二次凸优化问题，其中的对偶问题如下：

![](https://images2015.cnblogs.com/blog/1035514/201704/1035514-20170411191200672-1866658891.png)

求的的最终结果是：

![](https://images2015.cnblogs.com/blog/1035514/201704/1035514-20170411191237313-1973582067.png)

以上是线性可分的情况，在线性不可分的情况下就需要先扩维再计算，计算的形式是一样的：

![](https://images2015.cnblogs.com/blog/1035514/201704/1035514-20170411191307985-1594843862.png)

其中![](https://images2015.cnblogs.com/blog/1035514/201704/1035514-20170411191338501-340464698.png)表示的就是样本空间扩维以后的坐标。

​	从上述式子可以看出，无论是原始的样本空间还是扩维后的样本空间，在求解对偶问题时都需要各样本点的内积的结果。简单情况下只需将二维空间扩展到三维，但实际问题中往往需要扩到更高维的空间甚至是无穷维的空间才可以，这时候就涉及到计算内积的复杂度变高的问题，因此我们提出了核函数的概念。

​	通俗的理解核函数即为能使任意两个样本点在扩维以后的样本空间内的内积，和这两个样本在原来空间经过一个函数后的输出结果相同的函数。

​	核函数的确定并不困难,满足Mercer定理的函数都可以作为核函数。常用的核函数有：线性核函数，Sigmoid核函数和复合核函数，多项式核函数，傅立叶级数核，径向基核函数，B 样条核函数和张量积核函数等。

​	核函数方法具有非常广泛的应用，具有以下一些特点：

- 核函数的引入可以避免维数过高产生过高的计算量，而且输入空间的维数对核函数矩阵没有影响，因此核函数可以非常有效地处理高维输入；
- 不需要知道非线性变换函数Φ的形式和参数；
- 核函数的形式和参数的变化会对特征空间的性质产生影响，因此会产生不同核函数的性能变化；
- 核函数还可以与其他不同算法相结合，形成多种不同的基于核函数技术的方法，而且这两部分的设计可以分开进行，因此可以针对不同的应用选择不同的核函数和算法以达到最好的效果。

## 6.3.4实例演示

**线性可分演示：**

```python
输入：

from sklearn import svm

X = [[1, 0], [0, 1], [2, 2]]

y = [0, 0, 1]

clf = svm.SVC(kernel = 'linear')

clf.fit(X, y)

print('clf：',clf)
```

```python
结果：

clf： SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',max_iter=-1, probability=False, random_state=None, shrinking=True,tol=0.001, verbose=False)
```



参数解释：

（1）C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0；
（2）kernel：参数选择有RBF, Linear, Poly, Sigmoid, 默认的是"RBF";
（3）degree：if you choose 'Poly' in param 2, this is effective, degree决定了多项式的最高次幂；
（4）gamma：核函数的系数('Poly', 'RBF' and 'Sigmoid'), 默认是gamma = 1 / n_features;
（5）coef0：核函数中的独立项，'RBF' and 'Poly'有效；
（6）probablity: 可能性估计是否使用(true or false)；
（7）shrinking：是否进行启发式；
（8）tol（default = 1e - 3）: svm结束标准的精度;
（9）cache_size: 制定训练所需要的内存（以MB为单位）；
（10）class_weight: 每个类所占据的权重，不同的类设置不同的惩罚参数C, 缺省的话自适应；
（11）verbose: 跟多线程有关，不大明白啥意思具体；
（12）max_iter: 最大迭代次数，default = 1， if max_iter = -1, no limited;
（13）decision_function_shape ： ‘ovo’ 一对一, ‘ovr’ 多对多  or None 无, default=None
（14）random_state ：用于概率估计的数据重排时的伪随机数生成器的种子。
 ps：7,8,9一般不考虑。

在拟合之后，这个模型就可以用来预测：

```python
print('clf.predict([[1, 0]])：',clf.predict([[1, 0]]))

clf.predict([[1, 0]])： [0]
```

​	SVMs决策函数取决于训练集的一些子集，被称作支持向量。这些支持向量的属性可以在support_vectors_ ，support_ 和 n_support中找到。

```python
#get support vectors

print('clf.support_vectors：',clf.support_vectors)

#get indices of support vectors

print('clf.support：',clf.support) 

#get number of support vectors for each class

print('clf.n_support：',clf.n_support) 

clf.support_vectors_： [[ 1.  0.]

 [ 0.  1.]

 [ 2.  2.]]

clf.support_： [0 1 2]

clf.n_support_： [2 1]

clf.predict([[1, 0]])： [0]
```



# 6.4决策树

## 6.4.1模型简介

​        决策树模型是基于树理论实现的数据分类，很像数据结构中的B+树，是一种基本的分类和回归方法。决策树的模型是一颗树的形状结构，可以是二叉树也可以是多叉树。决策树中每个非结点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶结点存放一个类标签。使用决策树进行决策的过程就是从根结点开始，测试待分类项中相应的特征属性，并按照其值选择输出有向边，直到到达叶结点，也就是将实例分到叶结点的类中。决策树模型可以理解为对象属性和对象值之间的一种映射关系，学习得到的决策树既可以看作是多个 if-then 的规则，也可以看作是定义在特征空间与类空间上的条件概率分布。它对于噪声数据有很好的鲁棒性，并且能够从训练集中学习析取表达式。

​        决策树模型具有如下优点：

- 模型结构清晰，容易理解方便展示，并且可以提取出规则用作可视化分析；
- 标称型（有限的数据中取，如：是、否）和数值型（无限的具体的数据中取，如：1.23、4.56等）的数据可以同时处理；
- 对中间值缺失不敏感；
- 模型可以扩展到大型数据库中，且它的大小独立于数据库的大小。

​       决策树模型具有如下缺点：

- 容易出现过拟合的问题；
- 容易忽略数据本身属性之间的关联关系；
- ID3算法在计算信息增益时结果容易偏向数值比较多的特征。

​        决策树模型由于具有良好的分析能力、解释性等优点常被应用于企业管理实践，企业投资决策等决策应用中。

   决策树的核心树的分裂。到底该选择什么来决定树的分叉是决策树构建的基础。最好的方法是利用信息熵实现。熵这个概念很头疼，很容易让人迷糊，简单来说就是信息的复杂程度。信息越多，熵越高。所以决策树的核心是通过计算信息熵划分数据集。



## 6.4.2决策树相关概念





## 6.4.3常用决策树算法

**ID3算法**

​**C4.5算法** 

**CART算法**

​    



## 6.4.4决策树算法流程

**ID3算法流程**

输入:训练数据集 D，特征集 A，阈值; 

输出:决策树 T

**C4.5算法流程**

输入:训练数据集 D，特征集 A，阈值; 

输出:决策树 T 

**CART算法流程**

输入:训练数据集 D，停止计算的条件; 



## 6.4.5实例演示

