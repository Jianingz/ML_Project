# 7.3 卷积神经网络

## 目录
- 卷积运算
- 网络结构
- 卷积层
- 池化层
- 应用


卷积神经网络(Convolutuional Neural Network,简称CNN)，是深度学习技术中最具代表性的一种经典神经网络。卷积是一种特殊的线性运算，是值定义在两个联系实值函数的数学操作。而卷积神经网络就是指在网络结构中至少有一层使用了卷积运算来代替一般的矩阵乘法运算的神经网络。CNN应用非常广泛，尤其是在计算机视觉、图像识别等领域都有着出色的表现，获得了巨大成功。本节将对卷积神经网络做一个简介。


## 7.3.1 卷积运算

卷积操作是定义在两个连续实值函数上的数学操作，为了方便读者理解卷积，通过下面一个例子引出卷积的定义。(本例摘自参考文献《深度学习》)。

假设正在监控一艘自带激光传感器的宇宙飞船，传感器在任意时刻t都能产生一个输出x(t)，x(t)表示飞船在任意时刻t的位置，x和t都是实值。由于在实际过程中有噪声，传感器会受到不同程度的信号干扰，为了更加准确获得飞船的位置，可以对捕获的位置数据进行加权平均处理。显然，时间越近，捕获的数据与实际值约相关，所以不妨将最近的测量数据赋予更高的权值。我们可以通过加权函数w(a)实现，其中a表示测量结果距离当前时刻的时间间隔。这样加权平均后的飞船位置计算公式为：
![](/resource/7.3.1_1.jpg?raw=true)

这个运算就是连续卷积(convolution)的含义，可以表示为一个函数(w)在另外一个函数(x)上的加权叠加，通常被记为：
![](/resource/7.3.1_2.jpg?raw=true)

通常，我们把函数x称为输入(input)，函数w称为核函数(kernel)，输出则称为特征映射(feature map)。

卷积操作满足线性运算的三大规律，即交换律，分配律和结合律。令p=t-a，对于任意的t，当a趋于正无穷时，p趋近于负无穷，反之当a趋于负无穷时，p趋近于正无穷。从而有：
![](/resource/7.3.1_6.jpg?raw=true)

卷积的分配率和结合律可以表示为：
![](/resource/7.3.1_7.jpg?raw=true)

在上述例子中，激光传感器产生的是一个连续的数据，是随时间t不断变化的数值。但是在计算机的业务场景中，连续卷积无法实现，必须离散化，也就是以一定的时间间隔记录相邻的两个数据。还是借用上面的例子，如果激光传感器每隔时间间隔t(t为整数)发送一次数据，那么定义离散形式的卷积为：
![](/resource/7.3.1_3.jpg?raw=true)

如图所示，是二维空间上应用离散卷积的例子，离散卷积可以看作矩阵的乘法，卷积核是一个2*2的二维结构，将卷积核作用在输入数据上，按照从上到下，从左到右的顺序进行运算。
![](/resource/7.3.1_4.jpg?raw=true)

通过上面的例子，可以看到离散卷积本质是有加权的线性运算。在图片识别等领域中，通常也会配置一个宽高为w*h的卷积核，假设输入的图片大小为m*n。卷积核按照从上到下，从左到右的顺序扫描整个图片，会得到一个新的映射空间，其大小为(m-w+1)*(n-h+1)。

图展示了利用卷积核提取图像的特征，该卷积核是一个3*3的带权值的三维结构，被称为高斯-拉普拉斯算子，是在图像处理中非常著名的滤波器。
![](/resource/7.3.1_5.jpg?raw=true)


## 7.3.2 网络结构

在深入介绍卷积神经网络之前，我们先对CNN与传统的MLP在网络结构的异同点做一个简单的梳理。

首先是两者的相同点：

	- 两者都是多层的网络结构，每层的神经元有且只与前后层的神经元相关联，与非相邻层的神经元没有联系。此外每个神经元从上一层的神经元中接受数据，通过线性加权后，选择合适的非线性激活函数输出。
	- 两个都是从输入层输入数据，在输出层定义损失函数，并通过最小化损失函数调整权重以期得到最优解。

两者最大的区别在于MLP是一个全连接的网络，如图所示，传统神经网络每层的神经元的输入来自上一层所有神经元的数据，以此类推不断向后延伸。但是显然这样做有缺陷，比如局部最优解，梯度消失，可拓展性差，计算量大等。
![](/resource/7.3.2_8.jpg?raw=true)


CNN的出现在一定程度上对上述缺点做了改进，避免操作所有神经元，实现了端对端的表示学习思想。如图是一个典型的CNN神经网络结构，我们可以看到，卷积网络在进入全连接层之前，已经经过多个卷积层和池化层的处理，从而优化了网络结构。
![](/resource/7.3.2_9.jpg?raw=true)

简要概述CNN网络中重要的网络节点结构，详细内容将在后面各节中介绍。

	- 卷积层(Convolutional Layer)：CNN网络的核心结构之一，通过局部感知和参数共享两个原理，实现对高维数据的降维，从而提取图像特征。
	- 激活层(Activation Layer)：其作用是对上一层的线性输出做非线性化处理，之前章节介绍的激活函数，比如sigmoid激活函数，ReLU激活函数等都可以使用。
	- 池化层(Pooling Layer)：CNN网络的核心结构之一，可以把降维后的数据再次缩减数据规模，并且对输入数据具有局部线性转换的不变性，增强网络泛化能力。
	- 全连接层(Full Connected Layer)：经过卷积层和池化层的多层处理后，数据维度已经大幅下降，可以通过前馈网络处理；此外全连接层的输入是之前数据多次提炼后的抽象，比直接使用原始数据作为输入拥有更好的预测结果。


### 7.3.3 卷积层





