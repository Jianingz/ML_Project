# 7.3 卷积神经网络

## 目录
- 卷积运算
- 网络结构
- 卷积层
- 池化层
- 经典的卷积神经网络列举
- 应用


卷积神经网络(Convolutuional Neural Network,简称CNN)，是深度学习技术中最具代表性的一种经典神经网络。卷积是一种特殊的线性运算，是值定义在两个联系实值函数的数学操作。而卷积神经网络就是指在网络结构中至少有一层使用了卷积运算来代替一般的矩阵乘法运算的神经网络。CNN应用非常广泛，尤其是在计算机视觉、图像识别等领域都有着出色的表现，获得了巨大成功。本节将对卷积神经网络做一个简介。


## 7.3.1 卷积运算

卷积操作是定义在两个连续实值函数上的数学操作，为了方便读者理解卷积，通过下面一个例子引出卷积的定义。(本例摘自参考文献《深度学习》)。

假设正在监控一艘自带激光传感器的宇宙飞船，传感器在任意时刻t都能产生一个输出x(t)，x(t)表示飞船在任意时刻t的位置，x和t都是实值。由于在实际过程中有噪声，传感器会受到不同程度的信号干扰，为了更加准确获得飞船的位置，可以对捕获的位置数据进行加权平均处理。显然，时间越近，捕获的数据与实际值约相关，所以不妨将最近的测量数据赋予更高的权值。我们可以通过加权函数w(a)实现，其中a表示测量结果距离当前时刻的时间间隔。这样加权平均后的飞船位置计算公式为：
![](/resource/7.3.1_1.jpg?raw=true)

这个运算就是连续卷积(convolution)的含义，可以表示为一个函数(w)在另外一个函数(x)上的加权叠加，通常被记为：
![](/resource/7.3.1_2.jpg?raw=true)

通常，我们把函数x称为输入(input)，函数w称为核函数(kernel)，输出则称为特征映射(feature map)。

卷积操作满足线性运算的三大规律，即交换律，分配律和结合律。令p=t-a，对于任意的t，当a趋于正无穷时，p趋近于负无穷，反之当a趋于负无穷时，p趋近于正无穷。从而有：
![](/resource/7.3.1_6.jpg?raw=true)

卷积的分配率和结合律可以表示为：
![](/resource/7.3.1_7.jpg?raw=true)

在上述例子中，激光传感器产生的是一个连续的数据，是随时间t不断变化的数值。但是在计算机的业务场景中，连续卷积无法实现，必须离散化，也就是以一定的时间间隔记录相邻的两个数据。还是借用上面的例子，如果激光传感器每隔时间间隔t(t为整数)发送一次数据，那么定义离散形式的卷积为：
![](/resource/7.3.1_3.jpg?raw=true)

如图所示，是二维空间上应用离散卷积的例子，离散卷积可以看作矩阵的乘法，卷积核是一个2*2的二维结构，将卷积核作用在输入数据上，按照从上到下，从左到右的顺序进行运算。
![](/resource/7.3.1_4.jpg?raw=true)

通过上面的例子，可以看到离散卷积本质是有加权的线性运算。在图片识别等领域中，通常也会配置一个宽高为w*h的卷积核，假设输入的图片大小为m*n。卷积核按照从上到下，从左到右的顺序扫描整个图片，会得到一个新的映射空间，其大小为(m-w+1)*(n-h+1)。

图展示了利用卷积核提取图像的特征，该卷积核是一个3*3的带权值的三维结构，被称为高斯-拉普拉斯算子，是在图像处理中非常著名的滤波器。
![](/resource/7.3.1_5.jpg?raw=true)


## 7.3.2 网络结构

在深入介绍卷积神经网络之前，我们先对CNN与传统的MLP在网络结构的异同点做一个简单的梳理。

首先是两者的相同点：

	- 两者都是多层的网络结构，每层的神经元有且只与前后层的神经元相关联，与非相邻层的神经元没有联系。此外每个神经元从上一层的神经元中接受数据，通过线性加权后，选择合适的非线性激活函数输出。
	- 两个都是从输入层输入数据，在输出层定义损失函数，并通过最小化损失函数调整权重以期得到最优解。

两者最大的区别在于MLP是一个全连接的网络，如图所示，传统神经网络每层的神经元的输入来自上一层所有神经元的数据，以此类推不断向后延伸。但是显然这样做有缺陷，比如局部最优解，梯度消失，可拓展性差，计算量大等。
![](/resource/7.3.2_8.jpg?raw=true)


CNN的出现在一定程度上对上述缺点做了改进，避免操作所有神经元，实现了端对端的表示学习思想。如图是一个典型的CNN神经网络结构，我们可以看到，卷积网络在进入全连接层之前，已经经过多个卷积层和池化层的处理，从而优化了网络结构。
![](/resource/7.3.2_9.jpg?raw=true)

简要概述CNN网络中重要的网络节点结构，详细内容将在后面各节中介绍。

	- 卷积层(Convolutional Layer)：CNN网络的核心结构之一，通过局部感知和参数共享两个原理，实现对高维数据的降维，从而提取图像特征。
	- 激活层(Activation Layer)：其作用是对上一层的线性输出做非线性化处理，之前章节介绍的激活函数，比如sigmoid激活函数，ReLU激活函数等都可以使用。
	- 池化层(Pooling Layer)：CNN网络的核心结构之一，可以把降维后的数据再次缩减数据规模，并且对输入数据具有局部线性转换的不变性，增强网络泛化能力。
	- 全连接层(Full Connected Layer)：经过卷积层和池化层的多层处理后，数据维度已经大幅下降，可以通过前馈网络处理；此外全连接层的输入是之前数据多次提炼后的抽象，比直接使用原始数据作为输入拥有更好的预测结果。


### 7.3.3 卷积层

卷积层是CNN的核心，设计卷积层的核心目的在于降低数据维度，此外还有其他方面的考虑。所以在展开对卷积层的描述前，不妨先了解其设计目的。

目的1：实现对输入数据的降维

传统的神经网络有先天的缺陷，对数据规模非常敏感。一旦数据规模较大，由于每层的神经元数值的计算需要上一层所有神经元的参与，必然会造成计算缓慢甚至无法得出数据。比如传统的神经网络处理图片信息时，假设输入图片是1000*1000*3的RGB图像，那么输入层就有3*10^6的神经元，假设只有一层隐藏层，也拥有3*10^6的神经元，那么对于全连接的神经网络来说，从输入层到隐含层的参数就会有(3*10^6)^2=9*10^12个。可以看出，随着隐藏层数和输入图像复杂度的增加，训练参数将非常耗时甚至无法进行，所有需要对数据进行降维。

目的2：模拟生物学理论

早在19世纪60年代科学家提出了感受野的概念。当时科学家通过对猫的视觉皮层细胞的研究发现，每个神经元只会处理一小块区域的图像信息，这就是所谓的感受野。随后Hubel和Wiesel的研究表明，生物的视觉系统存在着复杂的细胞分布，当接受到外界的输入信号后，不同细胞对输入的不同部分具有局部敏感性。比如一些细胞对颜色数据敏感但是对边缘特征不敏感，这些细胞可以识别出图像的颜色但是却无法识别物体的边缘。再比如一些细胞对运动数据敏感但是对纹路数据不敏感，这些细胞可以识别整个视野的运动特征并忽略物体的纹路。视觉系统通过一个个感受野扫描整个视野，抽取特征信息，然后在通过对抽象出的特征进行组合，达到全局处理的效果。这也是神经网络的生物学基础。

目的3：提取深层特征

长期以来，科学家都从数学的角度强调卷积层的重要性，直到2014年，Zeiler和Fergus提出通过反卷积操作可视化每一层提取的特征。通过可视化，可以形象观察到CNN如何从提取第一层简单的边缘特征，在通过不断的抽象和组合出负责的形状特征。通过调整卷积核的参数，还可以看到CNN网络提取出了不一样的特征。如果再通过多个卷积核的组合，还能进一步加强CNN网络的表达能力，抽象出更复杂更贴近实际的特征，所以看出提取的深层特征对于图像的表征有非常明显的作用。

卷积层由三个部分组成，通过卷积核，不但可以提取出深层特征，还能极大程度减少参数数量，从而提高训练效率。

1.局部感知

在处理例如图像，音频等高纬数据时，如果神经元采用全接连的方式会导致训练时间和空间代价惊人。但是根据生物学中感受野的研究表明，动物对外界的认知并不是宏观的，一蹴而就的，相反是一个从局部到整体的过程。而图像的空间联系特征也是如此，局部之间的像素联系紧密，共同表达一个主题，距离较远的像素关联性就比较弱。所以，神经元并不需要把所有像素都做关联，只需要对局部的内容做感知去抽象即可。具体来说，就是在构建卷积层时，这一层中的每一个神经元并不需要与上一层的所有神经元发生联系，只需要一部分神经元即可。这种联系方式可以成为局部感知。

2.空间位置排列

卷积层的结构主要由四个超参数决定，包括卷积核的大小，卷积核的数量，步长和补零。下面围绕这四个参数详细介绍神经元是如何与输入层进行交互的。

卷积核的大小：卷积核的大小本质讲就是局部感知野的大小，卷积核是一块子区域，如图所示，定义了一个大小为5*5*3的卷积核，需要强调的是卷积核的大小主要针对宽度和高度，深度与原始数据保持一致。卷积层的每一个神经元只需要与卷积核对应的区域相关联。
![](/resource/7.3.3_10.jpg?raw=true)

卷积核的数量：每一个卷积核只能提取图像的一部分特征，但是显然这样做是不够的，提取的特征也不够充分，所以可以添加多个卷积核提取多重特征，每一个卷积核提取的特征都各有侧重点，这样做可以更加丰富的表征图像特征。而每一个卷积核与原始输入数据完成卷积操作后得到的神经元集合，就是特征图谱。

步长：指定卷积核在输入数据上移动的步幅，假设步长为s，那么卷积核在每一步会跳跃s个像素。一维数据为例，图X和图X分别展示了步幅为1和2的时候对应的神经元，假设卷积核大小为3，权重值为(1，0，-1)。
![](/resource/7.3.3_11.jpg?raw=true)

补零：在处理边界时常用到补零。由于卷积核的大小不一定被输入数据整除，必然有卷积核无法覆盖的地方。为了防止信息丢失，可以采用补零的策略，即在边界处适当补充零使得边界处的大小与卷积核吻合。例如，对于输入向量(0，4，1，-4，1，-3)，卷积核为(1，0，-1)，步长为2，那么卷积核不能完全覆盖输入数据。这时可以在原始数据的边界补充零，构造成(0，4，1，-4，1，-3，0)，这样卷积核就可以覆盖所有数据。

综上，构建卷积层时主要考虑卷积核的大小，卷积核的个数，步长和补零个数这四个方面。当这四个方面确定后，卷积层的结构也就唯一确定了。以一维数据为例，假设输入数据大小为w，卷积核的大小为f，步长为s，补零个数为p，那么对于每个卷积核，它与输入数据进行卷积运算后，得到的特征图谱包含的神经元个数为：
![](/resource/7.3.3_12.jpg?raw=true)

对于高纬数据，每一维度都可以参照上述公式计算。

3. 参数共享












