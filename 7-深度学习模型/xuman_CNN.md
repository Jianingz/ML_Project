[课程链接](http://cs231n.github.io/convolutional-networks/)

# CS231n Convolutional Neural Networks for Visual Recognition
## Convolutional Neural Networks (CNNs / ConvNets)
与普通神经网络相似，由学习权重和偏执的神经元组成。每个神经元接受一个输入，然后权重和输入进行点乘得到一个非线性的输出。
整个网络仍然是表示一个可微分的得分函数：从一个端点的像素输入到另一端的得分输出。
损失函数：在最后的全连接层计算损失函数。
## 架构概述 （Architecture Overview）
#### 普通神经网络：
接受一个输入，通过一系列的隐藏层对输入进行转换。隐藏层由一系列的神经元组成，每个神经元与前一层全连接，而且同一层的神经元彼此相互独立。最后一层称为全连接层，也叫做输出层。

![神经网络](https://github.com/xuman-Amy/CNN/blob/master/neural.png)

#### 三层卷积神经网络：
CNN由三维组成，宽度（width）、高度（height）、深度（depth），depth指激活维度，而不是神经网络的深度

![3层神经网络](https://github.com/xuman-Amy/CNN/blob/master/3_layer_network.png)

## Layers used to build ConvNets
CNN即一系列的层，每一层通过可微的函数将一个激活量转化成另一个。我们通过三个主要的层——卷积层（ Convolutional Layer），池化层（Pooling Layer），全连接层（Fully-Connected Layer）来构建一个CNN架构。
overview： [INPUT - CONV - RELU - POOL - FC]
* input: $$32\times32\times3$$的图片像素
* conv: 计算与输入层连接神经元的输出，将输入与权重进行点乘。如果过滤器有n个，则得到$$32\times32\times n$$的结果。
* relu:  用一个激活函数去作用于元素，比如是阈值为0的$$max(0，x)$$函数。这样不会改变数据的维度。
* pool:池化层即作用于空间维度的下采样的操作，减小数据的宽度和高度。
* FC(fully-connected):计算类别分数，返回$$1\times1\times10$$的结果，其中10代表10种类别的分数。
CON/FC层可以采用梯度递减训练数据保证分数是连续的。


接下来，分别介绍下每一个单独的层，包括超参数的细节和层之间的连通性。

## 卷积层（Convolutional Layer）
** 概述 **：将空间维度即宽*高的数据与过滤器相卷积
** 局部连通性 **：对于高纬度的输入数据，将每个神经元与所有的神经元相连接是不现实的，我们采用局部连通性，只连接输入数据一部分的神经元。这种连通性的空间扩展称为神经元的感受野，也就相当于过滤器的大小。深度与输入数据的深度相同。
![](blob:http://www.markdownnotes.com/866db938-04b9-40e9-8c2d-f5f0a3200a1e)

** 空间排列 **：决定输出数据的超参数有深度（depth），步数（strid）,零填充（zero-padding）.
1. depth:与使用的过滤器的个数有关。如如果卷积层的第一层将输入数据看作纯图像，那么沿着深度维度的所有神经元可能会在各个定向边缘或者各颜色通道被激活。我们将会指出一组神经元，他们将输入数据的同一区域看作深度列，或者称为纤维。
2. stride:过滤器每次移动的步数。
3. zreo-padding：有时将数据的边缘设置为0会比较方便。零填充有利于控制输出数据的空间维度，使其与输入数据的维度相同。

可以将输出数据的维度看作输入数据$$W$$,卷积神经元感受野$$F$$,步数stride$$S$$和边缘零填充量$$ P $$的函数.输出数据的维度计算公式为$$(W-F+2P)/S+1$$.例如输入数据$$7 \times 7$$,过滤器$$3 \times 3$$,stride = 1,pad = 0,输出数据维度为$$5 \times 5$$

![空间计算过程](https://github.com/xuman-Amy/CNN/blob/master/spacial.png)

**零填充的用法**如上左图中添加了零填充P = 2这样保证了输入数据和输出数据的维度相同，通常当stride = 1 时，$$P = (F-1)/2$$.
** stride的约束 **再次强调下空间排列的超参数是相互约束的。如果w=10,p=0,f=3,如果将s设置为2，输出为4.5，显然是不可以的。恰当的设置CNN网络的各参数，保证各个层可以更好的运行是很困难的一件事，零填充和遵循其他一些设计规则有益于框架的更好执行。
**参数共享**卷积层用于控制参数数量的方法。在同一depth的数据可以共享同一权重和偏置量。
**卷积层总结：**
* 输入数据	$$W_{1} \times H_{1} \times D_{1}$$
* 需要四个参数：
	*过滤器个数 K
	*空间扩展即感受野 F
	*步数 S
	*零填充数量 P
* 输出数据$$W_{2} \times H_{2} \times D_{2}$$
	* $$W_{2} = (W_{1} - F_{1} + 2P) / S + 1$$ 
	* $$H_{2} = (H_{1} - F_{1} + 2P) / S + 1$$ 
	* $$D_{2} = K$$ 
* 参数共享后，每一个过滤器引入 $$F\cdot F\cdot D_{1}$$ 个权重，卷积层一共引入$$(F\cdot F\cdot D_{1}) \cdot K$$ 个权重和K个偏置
* 对于输出量，第d层深度的值（$$W_{2} \times H_{2}$$ 大小）是第d个过滤器和输入数据的有效卷积在减去偏置量的值，其中卷积步长为S。
一般的卷积层参数设置为$$F =3, S = 1, P = 1$$
卷积过程demo:
![卷积过程](https://github.com/xuman-Amy/CNN/blob/master/convolution.png)
### 以矩阵相乘的方法来计算卷积
1. 将输入平铺成列向量
	通过$$im2col$$操作将输入数据平铺成列向量。假如输入数据是$$[227 \times 227 \times 3 ]$$,过滤器大小$$[11 \times 11 \times 3]$$，过滤步数为4。将过滤器的数据平铺为11*11*3=363的列向量。迭代这一过程，共$$（ 227 - 11 ）/4 +1$$ = 55个卷积块要在宽和高两个维度上进行，共55*55=3025次，所以卷积结束得到$$[363 \times 3025]$$的数据，称为$$ X\_col$$。
2. 同样，将卷积层的权重weight展开平铺成行向量。
	如果过滤器有96个，那么权重向量$$W\_row$$大小为$$ [96 \times 363]$$。
3. 卷积的结果就等同于进行矩阵$$W\_row，X\_col$$的相乘，$$np.dot(W\_row，X\_col)$$.在上述示例中，将得到$$[96 \times 3025]$$的结果。
4. 将最后的结果转化为$$[55 \times 55\times 96]$$的维度。
### 后向传播 backprogation

## 池化层（pooling layer）
通常是在连续的卷积层之间添加池化层，减少空间大小以及参数数量，减少神经网络的计算量，同时也防止过拟合。池化层在每个输入数据的每个深度切片上运用MAX操作相互独立运行。最通用的池化层用$$2 \times 2$$的过滤器，步数设为2，在输入数据的宽度和高度上分别进行下采样，丢掉75%的激活量。最大池化通常在四个数字进行，选取四个中的最大数。深度维度保持不变。
**池化层总结：**
* 输入$$W_{1} \times H_{1} \times D_{1}$$
* 需要参数
    *空间扩展即感受野 F
	*步数 S
* 生成的数据$$W_{2} \times H_{2} \times D_{2}$$
	* $$W_{2} = (W_{1} - F_{1} + 2P) / S + 1$$ 
	* $$H_{2} = (H_{1} - F_{1} + 2P) / S + 1$$ 
	* $$D_{2} = D_{1}$$
* 不引入其他参数
* 通常不需要零填充
### 一般的池化（general pooling）
除了最大池化之外，还有平均池化（average pooling）和L2-norm池化。以前经常使用平均池化，近几年最大池化更为流行，因为最大池化的性能更好。

![最大池化过程](https://github.com/xuman-Amy/CNN/blob/master/max_pooling.jpg)

### 去除池化层
有人提议去除池化层，可以通过加大步数stride控制数据量的大小。在去除池化层在生成型网络中起了=比较重要的作用，如GANs(generative adversarial networks)。

## 标准化层（normalization layer）
作用比较小，已经逐渐退出神经网络。

## 全连接层（fully-connected layer）
FC层的神经元与前一层的所有激活函数相连接。FC层的激活函数与矩阵相乘，然后抵消掉偏置量。
#### CONV层与FC层相互转换
二者最大的区别是CONV层的神经元只与前一层的部分激活函数相连接，而且参数是共享的。但是二者都是计算点乘，所以是可以相互转换的。
 * 对于每一个卷积层都有一个实现相同功能的前向全连接层。其中FC的权重矩阵会更大，且大部分都是0，因为CONV的局部连接性，但是在连接的区域，矩阵的值是相同的。
 * 相反的，对于每一个FC层，都可以转换为CONV层。例如感受野 F= 4096，输入为$$7 \times 7 \times\ 512$$,可以看作是$$F= 7,P = 0,S = 1, K = 4096$$的CONV层。也就是说我们将过滤器的大小设置为FC输入数据的大小，这样输出数据为$$1 \times 1 \times 4096$$,因为输入数据中只有一列适合。

$$FC>CONV 转换$$假如对于卷积框架的输入图像是$$224 \times 224 \times 3$$，采用卷积和池化之后为$$7 \times 7 \times 512$$。

## 卷积网络框架
首先是CONV-RELU层，接下来是池化层，重复这两层的叠加直至数据量比较小，之后通常加上一个全连接层。最后的FC通常会产生输出。



### 层大小
* 输入数据应该是可以被2整除的
* 卷积层应该用比较小的过滤器，通常设置步长为S=1。巧用零填充保证输入数据的大小不会被改变。比如感受野大小为F=3，则P=1；F=5，则P=2.一般来说，可以套用公式$$P = (F - 1 ) / 2$$.
* 池化层控制下采样。一般采用最大池化，选择$$2 \times 2$$的感受野，stride为2的过滤器，这样下采样为去除75%的激活量。

### 几种卷积网络架构
* [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)最佳用于读取压缩代码和数字。
* [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)最佳用于计算机视觉
* [ZF Net](http://arxiv.org/abs/1311.2901)AlexNet的改进版
* [GoogLeNet](http://arxiv.org/abs/1409.4842)优点在于大大减少了参数的数量。[最近一片GoogleNet的改进版](http://arxiv.org/abs/1602.07261)
* [VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)主要贡献是说明了网络的深度对于网络的性能是很重要的一个因素。
* [ResNet](http://arxiv.org/abs/1512.03385)


