## 7.1 神经网络简介（王佳琦）   

标签（空格分隔）： 未分类

　　神经网络作为人工智能的底层模型，在大数据时代应用十分普遍，很多应用都是基于神经网络的。比如模式识别、自动控制、深度学习等等。那么神经网络到底是用来干什么的呢？其实，它最常用的用途就是用来分类，在这里我们举几个直观的例子来说明：
　　

 - 非法广告的识别：如何判断一个广告是非法广告呢？一般需要将广告中出现的词汇提取出来，然后通过机器的判别，来决定这个广告是不是合法的。
 - 疾病诊断：，医院中的每一项检查都可能包含很多小“项目”，比如通过化验血、小便、以及肝功能、B超等检查，然后将检查结果送给机器去诊断，最后得出病人可能患有哪种疾病。
 - 狗品种分类：将许多品种的狗的图片送到机器中去训练，机器学习到哪种是哈士奇、泰迪或者雪纳瑞。
　　这种机器就叫做分类器。分类器的输入是一个向量，这个向量也叫做特征向量，就是向量中包含的是能代表事物特点的特征。分类器的输出是由数值来表示的。
　　比如在非法广告识别中，机器里本身就有一个“非法词库”，比如一些敏感词或者有色情意味的词，当把广告中的词提取出来之后，如果这些词在词库中出现，那么这个词在向量中对应的数字即为1，如果词库中没有该词，向量中对应的数字即为0；它的输出可以由0或者1表示，0就是代表非法广告，需要在最快时间下线，1代表合法广告，可以在广告位上展示。
　　在疾病诊断中，分类器的输入就是许许多多化验指标组成的向量；它的输出可以由0、1、2、3等数值表示，0可以代表身体健康，没有疾病，1可以代表有高血压，2可以代表糖尿病······
　　在狗品种分类中，分类器的输入是长度与图片像素和通道有关的向量，比如每一个图片都是像素为640*480的红绿蓝三通道图片，那么这个向量的长度就是640*480*3；同样输出是由数值表示，0代表哈士奇，1代表泰迪，2代表雪纳瑞······
　　分类器的目的是让分类尽可能的准确，一般我们会收集样本，并且已知这些样本的分类结果，这个分类结果也叫“标签”，换句话说就是将这些训练集打上标签。然后将它们“喂到”机器中去学习，也就是训练分类器，然后经过不断地调整优化，最后就可以将未知分类的样本输入到分类器中去预测分类结果了。
### 7.1.1	神经元与感知器
　　一直以来，人们希望可以模拟人类的大脑，以此创造出可以思考的机器。那么，人的大脑为什么能够进行思考呢？医学家发现，这是因为人体中含有无数个“神经网络”。
![](/resource/7.1.1_6.png?raw=true)
　　上图所示就是神经元的具体结构。神经元又叫神经细胞，外部刺激可以通过神经末梢的传导转化为电信号。神经中枢是由许许多多的神经元组成的，神经中枢对各种信号进行综合并作出相应的指令判断，而人体就依据神经中枢发出的指令做出相应的反应。
　　现在既然已经明白了人体中神经元的构造，那么如果能够构造出人造的神经元，就能构造出人工神经网络，这样就能模拟出人的大脑进行思考。在五十多年前的上世纪六十年代，有人提出了历史上最早的“人造神经元”模型，这个模型也叫“感知器”，伟大的是一直到现在这个概念依然沿用。
　　1958年，科学家Rosenblatt提出了“感知器”的概念（有些文献也叫作“感知机”），并定义感知器是由两层神经元组成的神经网络。这位伟大的科学家在当时甚至还现场演示了感知器学习识别简单图像的操作过程，作为当时第一个可以像人类一样“学习”的人工神经网络，这个创举在那时引起了巨大的反响。伟大的是，一直到现在这个概念依然沿用。
![](/resource/7.1.1_1.png?raw=true)
　　上图的圆圈就表示感知器，感知器的左边是输入，输入可以是一个也可以是多个，输出只有一个。神经元就相当于感知器，外部刺激相当于输入，外部刺激通过神经末梢的传导，输出为电信号的形式。
　　下面对感知器举一个例子，小张每周末要去户外打篮球，但是由于各种因素，小张不一定每周都去，影响小张去打球的因素有以下几点：
　　（1）	会不会下雨
　　（2）	会不会太晒
　　（3）	作业有没有写完
　　以上三个因素就构成了感知器的三个输入，最后的决定（去或者不去）就是输出。现实生活中，各个因素在人的心中的重要度很可能是不一样的，这里的重要度我们称之为“权值”并且一般来说所有权值之和为1。比如当不会下雨并且作业写完了的情况下，就算有一点晒，小张也会去打球；但是如果不晒并且作业写完了但是下了雨，小张一定就不回去打球。比如在小张心中，这三个输入的权值分别为0.6、0.1、0.3。这时还需要确定一个阈值k，如果最终输出大于阈值k，那么小张就会去打球。并且输出数值越大，表示想去打球的可能性越大。
　　上面的决策过程用数学公式表达如下：
![](/resource/7.1.1_2.png?raw=true)
　　也可以表示为：
![](/resource/7.1.1_3.png?raw=true)
　　其中ω表示决定输出的各个因素所对应的权值，x表示各个因素。
　　这里有人会问，如果输出只有两类——0或者1，当权值和阈值发生变化时，输出的敏感性会很低，所以当输出希望是一个连续的数值，而不是用二分类表示的时候，这是我们可以将其转换成一个连续的函数。
　　令z=ωx+b,这里引入sigmoid函数：
![](/resource/7.1.1_4.png?raw=true)
![](/resource/7.1.1_5.png?raw=true)
　　Sigmoid函数如上图所示。当感知器训练得非常成熟时，也就是z区域正无穷的时候，那么σ（z）→1；当感知器十分“青涩”没训练好的时候，那么σ（z）→0。也就是如果我们将σ（z）作为输出，输出结果就可以表示为连续函数了。
### 7.1.2神经网络简述
　　在现实生活中，一个决策模型可能包含很多感知器，也可以理解为决策模型是由感知器组成的多层网络。如下图所示，这个决策模型是由三层感知器组成的，最左边的一层感知器是输入层，它用来接收外部的输入，经过传导输出的内容作为第二层感知器的输入，直到上层感知器输出，第二层也叫隐藏层。第三层是输出层。这里每个感知器的输出依然只有一个，只不过是可以发送给多个感知器作为输入。可以直观的看出，输入层有四个单元，隐藏层有三个单元，输出层有两个单元。
![](/resource/7.1.2_1.png?raw=true)
　　搭建一个完整的神经网络需要满足以下三个条件：
　　（1）输入和输出
　　（2）权值和阈值
　　（3）多层感知器的结构
　　权值ω和阈值k都是人为确定的，但是怎么才能找到最合适的值呢？一般使用试错法，就是在其他参数不变的条件下，对ω或者k作出轻微的变动，记作∆ω或者∆k，接着观察输出的变化，不断的重复这个步骤，知道得出最适合的权值和阈值。这个过程就是模型的训练过程。
　　在构造一个神经网络的时候，输入层和输出层的感知器数量是一定的，在这里感知器也可以称之为“节点”，而隐藏层的节点数是可以变化的。整个神经网络的结构和箭头指向表示着预测过程中数据的流向，预测过程中数据的流向和大小与实际训练时候的情况可能有所不同。在训练神经网络的时候，我们的目的就是要找到最合适的权值，在图中就是表示为连接线的“大小”。
　　在训练过程中，感知器的权值是通过训练也就是不断调整得到的，这和神经元有所不同。我们可以把感知器看做一个逻辑回归模型，用来完成线性分类的任务。在这里我们用决策分解来表现分类的效果。当数据是二维平面时，可以用一条一维的直线将数据一分为二；当数据的维度是三维的时候，可以用一个二维的平面将数据分成两类······以此类推，当数据是n维的时候，可以用（n-1）的超平面将数据分为两类。如下图所示，用一条直线对二维平面进行决策分界。
![](/resource/7.1.2_2.png?raw=true)
　　2006年，Hinton在全世界非常有名的科学期刊《Science》上发表了一篇论文，在这篇论文中他提出了“深度训练网络”的概念。与之前的训练方式不同的是，“深度训练网络”中有一个步骤叫“预训练”（pre-training），这一步先在神经网络的权值最优解“附近”找到非常接近最优解的一个值，然后再使用“微调”的方法对整个神经网络进行训练。这两个技术大量减少了训练的时间。Hinton还将“深度学习”这个名字赋予给类似于这样的对多层神经网络进行训练优化的过程。
　　多层神经网络包含至少一层隐藏层。单层感知器只能学习线性函数，而多层感知器也可以学习非线性函数。线性代数是神经网络的重要理论基础之一，然而除了线性网络，神经网络中还有大量的非线性运算。比如对于隐藏层中的神经元，它的值再传给输出层之前，会先经过一次非线性运算。这里的非线性运算也叫“激活函数”，激活函数有很多种，最经典的就是sigmoid函数。
　　也许读者会问，为什么要引入非线性运算呢？最简单直接的原因就是，根据矩阵乘法的结合性，三个3*4与4*2与2*5的矩阵相乘相当于一个3*5的矩阵，那么如果多个线性运算层直接相连，就相当于只进行了一层线性运算，多层次的网络结构就失去了意义。
　　如果出现如下图所示的数据集，线性运算就无法“一刀”将之分割，而非线性运算就可以将直线变为曲线，把数据集分为两类，如下图所示。
![](/resource/7.1.2_3.png?raw=true)

### 7.1.3神经网络发展史
　　最近几年机器学习和人工智能的发展都渗透在我们生活的角角落落，而神经网络也是很多人第一次听说的一个名词。但是神经网络其实可以向前追溯到70多年前的1943年。为了更好地了解神经网络，我们这一节就来向读者们介绍一下神经网络的发展史。在神经网络发展的历史长河中，大约有三个里程碑事件，下面我们就来说一说这三个阶段。
　　（1）神经网络模型和感知机的提出
　　1943年两位德高望重的教授Warren McCulloch和Walter Pitts在在论文《A logical calculus of the ideas immanent in nervous activity》中提出了神经网络的数学模型，论文中提出的模拟大脑神经元的结构模型如7.1.1中所述，是利用各个输入与权重之积的和来模拟的，但是这个方法最大的缺点就是在调节权值的时候是手动的，所以工程量很大，还十分麻烦。
　　1958年Frank Rosenblatt教授提出了感知器模型，这个模型是一种二元分类器，在AI领域中，它也被定义为单层的人工神经网络。
　　在1969年，在以Marvin Minsky 和 Seymour Paper为作者的书《Perceptrons》中提出了感知机作为单层神经网络组成的缺点，因为感知机对一些线性不可分问题是束手无策的，比如异或问题。对于这个局限性，教授Marvin Minsky还提出了说法“感知器扩展出来的研究很可能要失败”。
　　当时Rosenblatt教授只是将感知机确认为单层神经网络的模型，并没有“举一反三”地推及到多层神经网络上，再加上《Perceptrons》这个书中的说法给社会带来的影响，神经网络领域的寒冬到来了，在长达十多年内的寒冬中，资金的冻结以及相关刊物的停止发表造成了神经网络的研究不得不暂停了下来。
　　直到80年代，有人提出了反向传播算法，同时人们也对多层感知机有了一定的认识，明白它没有单层感知机的问题。1987年《Perceptrons》改名为后再次出版《Perceptrons - Expanded Edition》，神经网络发展也迎来了第二个阶段。
　　（2）反向传播算法
　　随着分布式表达以及反向传播算法的提出，神经网络在上世纪80年代走出了寒冬。
　　分布式表达的含义就是可以通过多个神经元来表示生活中的许多概念，而多层神经元中的每一个神经元也需要加入到多个定义之中。分布式表达最大的优点就是极大的加强了模型的表达能力，上述提到的比如异或这种线性不可分问题可以利用分布式表达得以解决。
　　1982年，John J.Hopfield提出了Hopfield网络，John J.Hopfield教授是美国加州理工学院的物理学家。分布式表达和Hopfield网络让人们对神经网络的深入探索又燃起了兴趣。其中后者对后者对具有非线性连续变换函数的多层感知器的误差反向传播(Error Back Propagation)算法进行了详尽的分析，这里的误差反向传播就是大名鼎鼎的反向传播算法（Backpropagation algorithm，BP）。
　　多层感知器除了类似于单层感知器的输入、输出层之外，还有一个隐藏层。但是之前一直停留在无法直接获得隐藏层的权值的瓶颈上。而反向传播算法就是通过实际输出层的输出结果和期望输出的误差来间接地去调整隐藏层的权值。反向传播算法的学习过程可以分为两部分：信号的正向传播以及误差的反向传播，如下图所示。
　![](/resource/7.1.3_1.png?raw=true)
　　正向传播信息时，样本从输入层传入，经过各级隐藏层的层层处理之后由输出层传出，并求出实际输出值和期望输出值的误差,接着进入误差的反向传播过程。这是输出就通过隐藏层向反方向传播，在传播过程中将误差分散给各个层级，各个层级也就获得了各自的误差信息，这个误差信息就会成为更正各层级权值的依据。
　　没过多久，随之而来的问题就制约了反向传播算法的发展，这个问题就是在利用局部梯度下降的方法对权值进行调整的过程中，可能会出现梯度弥散（Gradient Diffusion）现象。
出现这种现象是因为在求导过程中，我们希望得到全局最优解，但是非凸目标代价函数容易陷入局部最优。并且如果神经网络的层数较多，这种情况会更加严重。再加上当时的数据量太少以至于无法进行深层网络训练等问题，神经网络逐步被支持向量机替代。
　　（3）深度学习
　　   Geoffrey Hinton是加拿大多伦多大学的教授，他在2006年对深度学习提出了连个观点，帮助反向传播算法走出了困境。这位教授在学术界大名鼎鼎的《科学》上发表了一篇期刊，其中提出的观点是：（1）多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；（2）对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。深度学习是一列在信息处理阶段利用非监督特征学习和模型分析分类功能的，具有多层分层体系结构的机器学习技术。深度学习的本质是对观察数据进行分层特征表示，实现将低级特征进一步抽象成高级特征表示。
　　在神经网络发展前期，计算量一直是阻碍自身发展的一个重要元素。随着计算机性能的不断发展，21世纪之后随着图形处理器（GPU）的出现，海量数据不再是神经网络发展的绊脚石。在这之中非常有名的一个事件是Stanford大学李飞飞教授建立的ImageNet项目，这个项目将图像和语义结合在一起，创造了至今为止世界最大的图像数据库——ImageNet，这个数据库由160多个国家的多达五万名工作者共同完成，其中的数据为深度学习提供足够的数量。Stanford大学还邀请Google、Microsoft等世界知名互联网企业使用他们的数据库，Stanford通过每年的一个比赛让这些IT大咖们利用这些数据为人类深度学习领域缔造更多的精彩。在这个比赛中踊跃出很多优秀的算法，其中最有名的就是2012年的竞赛冠军，它是第一个应用深度神经网络的算法——Alexnet，它的网络结构如下图所示：
　![](/resource/7.1.3_2.png?raw=true)
　　这个算法为什么会取得比较好的效果呢？一方面因为它使用了Relu激活函数，而基于RelU的深度卷积网络比基于tanh和sigmoid的网络训练速度快很多倍，另一方面利用了dropout的方法有效防止了神经网络的过拟合，dropout可以理解为一种正则化方法。
　　Alexnet算法取得的成绩对整个神经网络、深度学习领域都是一个里程碑式的纪念，随后各个领域也都取得了不同的成绩，比如语义分析、语音识别等等。
　　时间来到2016年，谷歌公司的黄士杰团队开发的一款人工智能围棋——阿尔法围棋（AlphaGo）战胜了世界围棋冠军李世石，其主要工作原理就是深度学习。这个伟大的节点又让人工智能开启了一个全新的篇章。
　　在现代社会中神经网络运用在生活的许多方面。在航空领域，我们可以将神经网络运用在自动驾驶中，其输入单元从汽车方向盘中读取信号和数据，输出单元会相应调整汽车的方向，保证汽车在安全的轨道行驶。在工厂中，我们可以将神经网络用于质量控制。比如，有家工厂通过大规模的复杂的化学处理过程生产洗衣液，我们可以测量出最终的洗衣液成分比例，将测量结果作为输入数据输入到神经网络中，然后神经网络就会决定是否进行化学处理。此外还有很多应用例子，这里不再一一举例。总之，有了神经网络，我们的生活变得越来越便捷化、智能化。神经网络让机器变得越来越像人类的大脑。
　　最后需要指出的一点是，虽然深度学习领域的研究人员相比于其他机器学习领域更多的受到大脑工作原理的启发，媒体也经常出于某种原因强调深度学习和大脑工作原理的相似性（可能这样显得更AI），但是现代深度学习的发展已经不完全是模拟人脑神经元的工作过程，或者可以说目前人类对大脑的工作机制的认知还不足以为当下的深度学习模型提供指导。	
　　
















