# 第四章 划分数据集  

标签（空格分隔）： 未分类

---
## 4.1 训练集/测试集
### 4.1.1 为什么划分
　　在没有机器学习和深度学习的时候，计算机没办法像人类一样智能的识别出事物，所以科学家们也在一直在这方面进行研究与探索。目前为止，他们也已经找到了一些方法，比如机器学习中的SVM以及深度学习，去让计算机能够拥有识别能力。  
　　那么有的读者就会问了，到底怎样利用数据去学习呢？举个简单的例子，假如我们需要去识别哈士奇这个品种的狗，所以我们需要用很多品种的狗的图片进行训练，这里的图片中的信息就可以理解为训练集。每个品种的狗的特征都不一样，他们的毛色、眼睛颜色、眼睛形状、体型等等特征造就了不同品种的狗。正是利用这些信息（数据）去学习哪些是哈士奇特有的特征，哪些不是它特有的特征。学习之后，再放入训练集没有的图片去测试，计算机就会利用已经学习到的哈士奇特有的特征去判断这个测试图片中的狗是否属于哈士奇。 
　　训练集的作用是用来找寻数据之间潜在关系的一组数据，当我们找到数据集中数据之间的关系之后，再用测试集去验证这个关系的强度，也就是预测的准确性。训练集和测试集之间是独立的，也就是他们之间没有重合的数据。并且二者的概率分布是一致的。
### 4.1.2 划分比例  
　　我们把将数据集划分成训练集、测试集这两部分的方法也叫Hold-out Validation。Hold-out Validation的原理很简单，并且它的运行速度也是十分快速的。但是由于它是基于强假设的，所以缺乏有效的统计特征，它的验证结果的可靠性就不是很高，同时也很难在单个数据集上计算方差信息与置信区间。所以一般来说如果数据及足够大，那么就可以用这种划分数据集的方法，那么预测值也相对来说比较可靠。
　　在几年前，科学家们还在运用一些传统方法的时候，往往按照7:3或者8:2的比例来划分训练集和测试集。而当数据量不大的时候，往往要再加一个验证集，它们三个的比例一般是训练集：验证集：测试集=6:2:2，如下图所示。 
![](/resource/4.1.1_1.png?raw=true)

　　然而到了现在这个时代，数据则很轻易就上升到了百万、千万的级别，那么这时就不需要那么多的验证集和测试集了。假设有200W的数据，只需要拿出1W的数据做验证集、1W的数据做测试集就可以了。也就是当数据集很大的时候，训练集、验证集、测试集的比例可以是98:1:1，也可以是998:1:1，具体比例可以根据实际情况来划分。
　　在将数据集划分之后，我们选择一个我们认为适合给定问题的学习算法。首先我们需要设定超参数，超参数是我们学习算法的参数，它也叫元参数。我们必须手动设定这些超参数值，因为学习算法不会从训练数据中学习这些值。由于在模型拟合过程中没有学习超参数，所以我们需要某种“额外的过程”或“外部循环”来分别优化它们——这种坚持的方法不适合这个任务。因此我们可以利用现有的机器学习库、经验或者现成算法的默认参数，来设定这些固定的超参数值。	
　　接下来的一个问题就是怎样才能说明模型的性能是“好”的呢?这就是独立测试集发挥作用的地方。由于学习算法以前从未“见过”这个测试集，因此它应该对它在未见的新数据上的性能提供一个相对公正的估计。现在，我们使用这个测试集并使用模型来预测类标签。然后，我们将预测的类标签与正确的类标签进行比较，以估计模型的泛化精度或误差。
### 4.1.3 层次，非层次划分 
　　如果一个数据集在随机抽样之前有很高的等级不平衡，那么在最坏的情况下，经过随机抽样，测试集可能不包含任何少数类的实例，也就是这会严重增加预测结果的偏差。因此,推荐的做法是用分层的方式划分数据集。在这里，分层仅仅意味着我们随机地分割一个数据集，这样每个类在结果子集(训练和测试集)中得到正确的表示，换句话说，分层是在结果子集中保持原来的类比例的方法。也就是如果样本集中存在好几个类别，那么在划分训练集、测试集的时候也要按照类别的比例去划分。那么我们也就知道，非层次划分就是不需要按照类别比例划分，而是随即在所有数据集中抽取即可。比如有两类，15个白球，5个黑球。那么层次划分就是按照80%、20%的比例从白球、黑球中分别抽取训练集、测试集，将白球的训练集和黑球的训练集合在一起作为总训练集，同理总测试集。非层次划分就是把白球和黑球合在一起，在这20个球里面按照80%、20%的比例去划分训练集和测试集。
　　非层次划分如果只做一次分割，它对训练集、验证集和测试集的样本数比例，还有分割后数据的分布是否和原始数据集的分布相同等因素比较敏感，不同的划分会得到不同的最优模型，而且分成三个集合后，用于训练的数据更少了。
### 4.1.4python代码及常用库

　　举个例子，总共有1000条数据
```
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn import svm
iris = datasets.load_iris()
iris.data.shape, iris.target.shape
((1000, 2), (1000,))

```
　　用 train_test_split 来随机划分数据集，其中 20% 用于测试集，有 200 条数据，80% 为训练集，有 800 条数据：
```
 X_train, X_test, y_train, y_test = train_test_split(
iris.data, iris.target, test_size=0.2, random_state=0)
	
 X_train.shape, y_train.shape
((200, 2), (200,))  
X_test.shape, y_test.shape
((200, 2), (200,))

```
用 train 来训练，用 test 来评价模型的分数。
```
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)                            
0.96...

```

## 4.2 交叉验证

### 4.2.1交叉验证简介
　　交叉验证的英文全称是Cross Validation，它是不同于Holdout Validation的另一种划分数据集的方法。交叉验证的基本思想是在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为测试集(validation set or test set),首先用训练集对分类器进行训练,再利用测试集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。
　　交叉验证背后的主要思想是，数据集中的每个示例都有被训练和测试的机会。k -fold交叉验证是交叉验证的一种特殊情况，我们在数据集中迭代k次。在每轮迭代中,我们将数据集分为k个部分:,k-1个部分合并成一个训练子集，剩下的一个部分作为测试集。
　　交叉验证其实分为两类，一类叫k折交叉验证法（k-fold Cross Validation）,还有一类叫留一交叉验证法（Leave one out cross validation）。后者可以理解为前者的特殊情况，即假设样本集中有m条样本，而当k=m时，所用的方法就叫留一交叉验证法。这种方法的每轮迭代中，测试集都只有一条样本，总共要进行m轮迭代，这个方法用于训练的数据只比整体数据集少了一个样本，因此最接近原始样本的分布。但是由于模型的数量与原始数据样本数量相同，所以训练复杂度十分大，因次如果在原始数据极度缺乏的情况，可以使用这种方法，普遍情况下还是使用k折交叉验证法比较多。
### 4.2.2目的
　　我们为什么要用交叉验证法呢？因为当数据集较小的时候，我们通过交叉验证相当于“扩大”了数据集，也就在一定程度上避免了过拟合，还可以从有限的数据中获取更多的有用信息。另外，由于交叉验证是从很多不同的方向去学习数据，所以交叉验证就能避免陷入局部最小值。
　　这三个原因就是使用交叉验证的目的。
### 4.2.3划分的原理详解图
　　如下图所示,它表示了5折交叉验证。
　　![](/resource/4.2.3_1.png?raw=true)
　　如上图所示，在k折的交叉验证中，这个过程将生成k个不同的模型，这些模型的训练集是不完全相同但部分重叠的，并在不重叠的验证集上进行评估。最后，我们计算交叉验证性能的算数平均值作为整个模型的性能的最终值。其详细步骤如下表所示：
　　![](/resource/4.2.3_2.png?raw=true)
　　　
### 4.2.4层次，非层次划分
　　交叉验证中的层次/非层次划分的含义和4.1中的一样。在交叉验证中的层次划分就是在每一份子集中都保持和原始数据集相同的类别比例。
### 4.2.5数据量和划分叠数的关系
　　5折交叉验证是最常用的，当然，当数据量较大的时候，我们也可以选择3折交叉验证；当数据量较小的时候，我们可以选择10折交叉验证。
### 4.2.6python代码及常用库
　　sklearn更新之前交叉验证在cross_validation模块里面，更新之后在model_selection里面。
　　Sklearn的功能十分强大，它提供的交叉验证函数也非常多，比如：
```
'BaseCrossValidator',
'GridSearchCV',
'TimeSeriesSplit',
'KFold',
'GroupKFold',
'GroupShuffleSplit',
'LeaveOneGroupOut',
'LeaveOneOut',
'LeavePGroupsOut',
'LeavePOut',
'ParameterGrid',
'ParameterSampler',
'PredefinedSplit',
'RandomizedSearchCV',
'ShuffleSplit',
'StratifiedKFold',
'StratifiedShuffleSplit',
'check_cv',
'cross_val_predict',
'cross_val_score',
'fit_grid_point',
'learning_curve',
'permutation_test_score',
'train_test_split',
 'validation_curve'

```
　　最简单的方法是直接调用 cross_val_score，这里用了 5 折交叉验证：
　　第一步就是加载数据集
```
import numpy as np 
from sklearn import cross_validation
from sklearn import datasets
from sklearn import svm
iris.data.shape, iris.target.shape
((150, 4), (150,))

```
经过第一步之后，数据集特征和类标签分别为iris.data, iris.target，第二步开始交叉验证。
```
X_train, X_test, y_train, y_test = cross_validation.train_test_split(
...     iris.data, iris.target, test_size=0.4, random_state=0)
X_train.shape, y_train.shape
((90, 4), (90,))
X_test.shape, y_test.shape
((60, 4), (60,))
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)                           
0.96...

```
其中第七行的clf是分类器的意思，可以自己任意选择，举个例子如果选择随机森林：
```
clf = RandomForestClassifier(n_estimators=400)
```




##  4.3 项目实例运用
　　这里我们再举一个十折交叉验证的例子，所以需要把数据集复制10份，并且用十个文件夹来存放它们，每个文件夹里面包含训练集（train）和测试集（test）。
```
# -*- coding: utf-8 -*-
# @Date    : 2017-05-11 21:24:50
# @Author  : Alan Lau (rlalan@outlook.com)
# @Version : Python3.5

from fwalker import fun
from bfile import buildfile as bf
from random import shuffle
import numpy as np
import shutil
import os

def buildfile(output_path):
    for i in range(1, 10+1):
        # 循环新建编号1-10的文件夹，用于存放train和test文件夹
        file_num = bf('%s\\%d' % (output_path, i))
        # 在每个编号文件夹下新建train文件夹，用于存放90%的训练数据
        train_file = bf('%s\\train' % file_num)
        # 在每个编号文件夹下新建test文件夹，用于存放10%的训练数据
        test_file = bf('%s\\test' % file_num)
    print('Data storage has been bulit!')
    return output_path


def split_ten(files):
    file_len = len(files)  # 获取文件总数
    shuffle(files)  # 随机打乱文件路径列表的顺序，即使python的随机是伪随机
    data_storage = []  # 初始化一个列表，用来接收分划分好的文件路径
    remainder = file_len % 10  # 判断文件数量能否直接被10整除
    if remainder == 0:  # 如果可以整除，直接将数据切成10组
        np_files = np.array(files)  # 将文件路径列表转换成numpy
        data_storage = np_files.reshape(10, -1)  # 利用numpy的reshape来将文件路径切分为10组
        # 比如说现在有20个文件路径
        # reshape()后得到的结果为2、2、2、2、2、2、2、2、2、2，即共十份、每份包含2个文件路径。
        return data_storage
    else:  # 否则，则先切开余数部分的文件
        np_files = np.array(files[:-1*remainder])  # 切开余数部分的文件，使文件数量保证能够被10整除
        data_storage_ten = np_files.reshape(10, -1)  # 同样利用上面的方法使用numpy切分10组文件
        # 获取余数部分的文件列表，遍历列表，尽可能的将多余的文件分散在10组文件中，而不是直接加入到一个文件中
        remainder_files = (
            np.array(files[-1*remainder:])).reshape(remainder, -1)  # 使用reshape切分问一份一组
        for i in range(0, len(remainder_files)):
            ech_dst = data_storage_ten[i]
            ech_rf = remainder_files[i]
            # 将取出来的余数内的路径分别加入到已经均分好的10份的前remainder个数据当中，比如说现在有24份文件，
            # 将24拆份拆分成一个能被10整除的数和一个余数，即这里拆分成20和4，我们首先将拆出来的20份文件均分10份，
            # 即每份有2个文件路径，然后，再将剩下后面的4个文件路径，尽可能的放入到刚刚均分好的10份数据中。
            # 因此最终拆分的结果共有十份，每份数量分别为：3、3、3、3、2、2、2、2、2、2。
            data_storage.append(np.concatenate((ech_dst, ech_rf)))
        for j in range(remainder, len(data_storage_ten)):
            # 将将剩下的没有被余数部分加入的份加入到data_storage中
            data_storage.append(data_storage_ten[j])
        return np.array(data_storage)


def group_data(data_storage, output_path):
    for i in range(0, len(data_storage)):
        ech_path = '%s\\%d' % (output_path, i+1)  # 构造每一份需要写入的路径
        ech_train_path = '%s\\train' % ech_path
        ech_test_path = '%s\\test' % ech_path
        test_paths = data_storage[i]
        move_file(test_paths, ech_test_path)
        train_paths = np.concatenate(([data_storage[:i], data_storage[i+1:]]))
        # 将剩下的训练部分加入到train_paths中，并且降维
        train_paths = np.concatenate((train_paths))  # 再次降维，使其变成1维
        move_file(train_paths, ech_train_path)
        num = i+1
        print('No.%d is over!' % num)

def move_file(old_paths, new_path):
    for old_path in old_paths:
        shutil.copy2(old_path, new_path)
        flag_name = '_'.join(old_path.split('\\')[-2:])
        old_name = '%s\\%s' % (new_path, old_path.split('\\')[-1])
        new_name = '%s\\%s' % (new_path, flag_name)
        os.rename(old_name, new_name)


def main():
    file_path = r'..\data\data_of_movie'
    # file_path = r'..\data\test'
    output_path = r'..\data\tenTimesTraining'
    files = fun(file_path)
    output_path = buildfile(output_path)
    data_storage = split_ten(files)
    group_data(data_storage, output_path)


if __name__ == '__main__':
    main()


```





