# 第四章 划分数据集 

标签（空格分隔）： 未分类

---
## 4.1 训练集/测试集
### 4.1.1 为什么划分
　　在没有机器学习和深度学习的时候，计算机没办法像人类一样智能的识别出事物，所以科学家们也在一直在这方面进行研究与探索。目前为止，他们也已经找到了一些方法，比如机器学习中的SVM以及深度学习，去让计算机能够拥有识别能力。  
　　那么有的读者就会问了，到底怎样利用数据去学习呢？举个简单的例子，假如我们需要去识别哈士奇这个品种的狗，所以我们需要用很多品种的狗的图片进行训练，这里的图片中的信息就可以理解为训练集。每个品种的狗的特征都不一样，他们的毛色、眼睛颜色、眼睛形状、体型等等特征造就了不同品种的狗。正是利用这些信息（数据）去学习哪些是哈士奇特有的特征，哪些不是它特有的特征。学习之后，再放入训练集没有的图片去测试，计算机就会利用已经学习到的哈士奇特有的特征去判断这个测试图片中的狗是否属于哈士奇。 
　　训练集的作用是用来找寻数据之间潜在关系的一组数据，当我们找到数据集中数据之间的关系之后，再用测试集去验证这个关系的强度，也就是预测的准确性。训练集和测试集之间是独立的，也就是他们之间没有重合的数据。并且二者的概率分布是一致的。
### 4.1.2 划分比例  
　　我们把将数据集划分成训练集、测试集这两部分的方法也叫Hold-out Validation。Hold-out Validation的原理很简单，并且它的运行速度也是十分快速的。但是由于它是基于强假设的，所以缺乏有效的统计特征，它的验证结果的可靠性就不是很高，同时也很难在单个数据集上计算方差信息与置信区间。所以一般来说如果数据及足够大，那么就可以用这种划分数据集的方法，那么预测值也相对来说比较可靠。
　　在几年前，科学家们还在运用一些传统方法的时候，往往按照7:3或者8:2的比例来划分训练集和测试集。而当数据量不大的时候，往往要再加一个验证集，它们三个的比例一般是训练集：验证集：测试集=6:2:2，如下图所示。 
  
![](/resource/4.1.1_1.png?raw=true)

　　然而到了现在这个时代，数据则很轻易就上升到了百万、千万的级别，那么这时就不需要那么多的验证集和测试集了。假设有200W的数据，只需要拿出1W的数据做验证集、1W的数据做测试集就可以了。也就是当数据集很大的时候，训练集、验证集、测试集的比例可以是98:1:1，也可以是998:1:1，具体比例可以根据实际情况来划分。
　　在将数据集划分之后，我们选择一个我们认为适合给定问题的学习算法。首先我们需要设定超参数，超参数是我们学习算法的参数，它也叫元参数。我们必须手动设定这些超参数值，因为学习算法不会从训练数据中学习这些值。由于在模型拟合过程中没有学习超参数，所以我们需要某种“额外的过程”或“外部循环”来分别优化它们——这种坚持的方法不适合这个任务。因此我们可以利用现有的机器学习库、经验或者现成算法的默认参数，来设定这些固定的超参数值。	
　　接下来的一个问题就是怎样才能说明模型的性能是“好”的呢?这就是独立测试集发挥作用的地方。由于学习算法以前从未“见过”这个测试集，因此它应该对它在未见的新数据上的性能提供一个相对公正的估计。现在，我们使用这个测试集并使用模型来预测类标签。然后，我们将预测的类标签与正确的类标签进行比较，以估计模型的泛化精度或误差。
### 4.1.3 层次，非层次划分 
　　如果一个数据集在随机抽样之前有很高的等级不平衡，那么在最坏的情况下，经过随机抽样，测试集可能不包含任何少数类的实例，也就是这会严重增加预测结果的偏差。因此,推荐的做法是用分层的方式划分数据集。在这里，分层仅仅意味着我们随机地分割一个数据集，这样每个类在结果子集(训练和测试集)中得到正确的表示，换句话说，分层是在结果子集中保持原来的类比例的方法。也就是如果样本集中存在好几个类别，那么在划分训练集、测试集的时候也要按照类别的比例去划分。那么我们也就知道，非层次划分就是不需要按照类别比例划分，而是随即在所有数据集中抽取即可。比如有两类，15个白球，5个黑球。那么层次划分就是按照80%、20%的比例从白球、黑球中分别抽取训练集、测试集，将白球的训练集和黑球的训练集合在一起作为总训练集，同理总测试集。非层次划分就是把白球和黑球合在一起，在这20个球里面按照80%、20%的比例去划分训练集和测试集。
　　非层次划分如果只做一次分割，它对训练集、验证集和测试集的样本数比例，还有分割后数据的分布是否和原始数据集的分布相同等因素比较敏感，不同的划分会得到不同的最优模型，而且分成三个集合后，用于训练的数据更少了。
### 4.1.4python代码及常用库

　　举个例子，总共有1000条数据
```
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn import svm
iris = datasets.load_iris()
iris.data.shape, iris.target.shape
((1000, 2), (1000,))

```
　　用 train_test_split 来随机划分数据集，其中 20% 用于测试集，有 200 条数据，80% 为训练集，有 800 条数据：
```
 X_train, X_test, y_train, y_test = train_test_split(
iris.data, iris.target, test_size=0.2, random_state=0)
	
 X_train.shape, y_train.shape
((200, 2), (200,))  
X_test.shape, y_test.shape
((200, 2), (200,))

```
用 train 来训练，用 test 来评价模型的分数。
```
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)                            
0.96...

```
　　　
　　


## 4.2 交叉验证

### 4.2.1是什么
### 4.2.2目的
### 4.2.3划分的原理详解图
### 4.2.4层次，非层次划分
### 4.2.5数据量和划分叠数的关系
### 4.2.6python代码及常用库

##  4.3 项目实例运用






