庄周
# 3.1 特征聚合

机器学习越来越多地从人工设计模型转向使用 H20、TPOT 和 auto-sklearn 等工具自动优化的工具。这些库以及随机搜索等方法旨在通过寻找匹配数据集的最优模型来简化模型选择和机器学习调优过程，而几乎不需要任何人工干预。然而，特征工程作为机器学习流程中可能最有价值的一个方面，几乎完全是人工的。  
特征工程也被称为特征构造，是从现有数据中构造新的特征从而训练机器学习模型的过程。这一步可能比实际上使用的模型更重要，因为一个机器学习算法只能从我们给定的数据中学习，所以构造一个和任务相关的特征是至关重要的。通常，特征工程是一个冗长的人工过程，依赖于领域知识、直觉和数据操作。这个过程可能是极其枯燥的，同时最终得到的特征将会受到人的主观性和时间的限制。特征工程自动化旨在通过从数据集中自动构造候选特征，并从中选择最优特征用于训练来帮助数据科学家。  
在本文中，我们将介绍一个使用 Feature Tools Python 库实现特征工程自动化的例子。我们将使用一个示例数据集来说明基本概念（继续关注之后使用真实世界数据的例子）。  
## 3.1.1 特征工程基本概念  
特征工程意味着从现有的数据中构造额外特征，这些特征通常分布在多张相关的表中。特征工程需要从数据中提取相关信息并将其存入单张表格中，然后被用来训练机器学习模型。构造特征是一个非常耗时的过程，因为每个新的特征通常需要几步才能构造，特别是当使用多张表的信息时。我们可以将特征构造的操作分为两类：「转换」和「聚合」。以下通过几个例子来看看这些概念的实际应用。通过从一或多列中构造新的特征，「转换」作用于单张表 。举个例子，若有如下的某一个县城客户表：  
![image]()

我们可以通过查找 joined 列中的月份或是自然对数化 income 列的数据来构造新的特征。这些都是转换操作，因为它们只用到了一张表的信息。

另一方面，「聚合」是跨表实现的，并使用一对多的关联来对观测值分组，然后计算统计量。例如，若我们有另外一张包含另一个县城客户贷款信息的表格，其中每个客户可能有多项贷款，我们便可以计算每个客户贷款的平均值、最大值和最小值等统计量。这个过程包括根据不同客户对贷款表进行分组并计算聚合后的统计量，然后将结果整合到客户数据中，构成整个市级单位的表格。


这些操作本身并不困难，但是如果有数百个变量分布在数十张表中，这个过程将无法通过人工完成。理想情况下，我们希望有一个解决方案能够在不同表间自动执行转换和聚合操作，并将结果整合到一张表中。  
## 3.1.2特征聚合的目的
目的：
数据简化：通过将表格数据合并，减少特征数。  
变换尺度：低级数据向高级数据转化，例如，将县级银行贷款数据转化为市级银行贷款。  
更加稳定的数据：使数据有更少的变化，例如将月份数据转化为年数据。  
## 3.1.3特征工具  
幸运的是，Feature Tools 正是我们正在找寻的解决方案。这个开源的 Python 库可以从一组相关的表中自动构造特征。特征工具基于名为「深度特征合成」的方法。深度特征合成叠加多个转换和聚合操作，这在特征工具的词库中被称为特征基元，以便通过分布在多张表内的数据来构造新的特征。与机器学习中的大多数方法一样，这是建立在简单概念基础之上的复杂方法。通过一次学习一个构造块，我们可以很好地理解这个强大的方法。首先，让我们看一下示例数据。我们已经看到了上面的一些数据集，并且完整的表组如下所示：  
clients： 关于信用社客户的基本信息。每个客户只对应数据框中的一行。  
loans： 向用户提供的贷款。每项贷款只对应数据框中的一行，但是客户可能有多项贷款。  
payments：贷款还本的支付。每笔支付只对应一行，但是每项贷款可以有多笔支付。    
如果我们有一个机器学习任务，例如预测客户未来是否会偿还一项贷款，我们希望将所有关于客户的信息整合到一张表中。这些表是相关的（通过 client_id 和 loan_id 变量），并且我们可以通过一系列转换和聚合操作来人工实现这个过程。然而，我们很快就可以使用特征工具来自动实现这个过程。  
**实体和实体集**  
特征工具的前两个概念的是「实体」和「实体集」。一个实体就是一张表（或是 Pandas 中的一个 DataFrame（数据框））。一个实体集是一组表以及它们之间的关联。将一个实体集看成另一种Python数据结构，并带有自己的方法和属性。  
现在我们需要整合两个实体。每个实体都必须带有一个索引，它是一个包含所有唯一元素的列。就是说，索引中的每个值只能在表中出现一次。在 clients 数据框中的索引是 client_id，因为每个客户在该数据框中只对应一行。  
loans 数据框还有另外一个唯一的索引，loan_id，同时将其添加到实体集的语法与 clients 一样。然而，payments 数据框不存在唯一索引。当我们把 payments 数据框添加到实体集中时，我们需要传入参数make_index =True，同时指定索引的名字。另外，尽管特征工具能自动推断实体中每列的数据类型，但是我们可以通过将列数据类型的字典传递给参数 variable_types 来覆盖它。  
列的数据类型已根据我们指定的修正方案被正确推断出来。接下来，我们需要指定实体集中表是如何关联的。  
**表的关联**  
考虑两张表之间「关联」的最好方法是类比父子之间的关联。这是一种一对多的关联：每个父亲可以有多个儿子。对表来说，每个父亲对应一张父表中的一行，但是子表中可能有多行对应于同一张父表中的多个儿子。  
例如，在我们的数据集中，clients 数据框是 loans 数据框的一张父表。每个客户只对应 clients 表中的一行，但是可能对应 loans 表中的多行。同样，loans 表是 payments 表的一张父表，因为每项贷款可以有多项支付。父亲通过共享变量与儿子相关联。当我们执行聚合操作的时候，我们根据父变量对子表进行分组，并计算每个父亲的儿子的统计量。  
该实体集现在包含三个实体（表），以及将这些表连接在一起的关联规则。在添加实体和形式化关联规则之后，实体集就完整了并准备好从中构造新的特征。  
**特征基元**  
在我们深入了解深度特征合成之前，我们需要了解特征基元的概念。我们其实早就知道是什么了，只是我们刚刚用不同的名字来称呼它们！它们只是我们用来构造新特征的操作：  
聚合：根据父与子（一对多）的关联完成的操作，也就是根据父亲分组并计算儿子的统计量。一个例子就是根据 client_id 对 loan 表分组并找到每个客户的最大贷款额。  
转换：对一张表中一或多列完成的操作。一个例子就是取一张表中两列之间的差值或者取一列的绝对值。  
在特征工具中单独使用这些基元或者叠加使用这些基元可以构造新的特征。以下是特征工具中一些特征基元的列表，也可以自定义特征基元。

这些基元可以单独使用或是组合使用以构造新的特征。为了使用特定的基元构造新的特征，我们使用 ft.dfs 函数（代表深度特征合成）。我们传入 entityset 和 target_entity，这是我们想要在其中添加特征的表，被选参数 trans_primitives（转换）和 agg_primitives（聚合）。

返回的是包含每个客户新特征的数据框（因为我们定义客户为 target_entity）。比如我们有每个客户加入的月份，这是一个转换操作的特征基元：

我们也有许多聚合操作的基元，比如每个客户的平均支付总额：

尽管我们仅指定了一些特征基元，但是特征工具可以通过组合和叠加这些基元来构造新的特征。

完整的数据框包含 793 列的新特征！  
**深度特征合成**  
我们现在具备理解深度特征合成（dfs）的一切条件。事实上，我们已经在前面的函数调用中执行了 dfs！深度特征只是叠加多个基元构造的一个特征，而 dfs 只是构造这些特征的过程的名称。深度特征的深度是构造这个特征所需的基元数量。  
例如，MEAN（payments.payment_amount）列是深度为 1 的特征，因为它是使用单个聚合操作构造的。LAST（loans（MEAN（payments.payment_amount））是一个深度为 2 的特征，它是由两个叠加的聚合操作构造的：MEAN 列之上的 LAST（最近的）列。这表示每个客户最近的贷款平均支付额。

我们可以叠加任意深度的特征，但在实践中，我从没有使用超过 2 个深度的特征。此外，这些特征很难解释，但是我鼓励任何对「深入」感兴趣的人。  
我们不必人工指定特征基元，但可以让特征工具自动为我们选取特征。为此，我们使用相同的 ft.dfs 函数调用，但是不传入任何特征基元。  
特征工具构造了很多特征供我们使用。尽管这个过程确实能自动构造新的特征，但是它不会取代数据科学家，因为我们仍然需要弄清楚如何处理这些特征。例如，我们的目的是预测一位客户是否会偿还贷款，我们可以寻找与特定结果最相关的特征。此外，如果我们具有领域知识，我们可以用这些知识来选择指定的特征基元或候选特征的种子深度特征合成。  
特征工程自动化解决了一个问题，但是带来了另一个问题：特征太多了。尽管在拟合一个模型之前很难说哪些特征是重要的，但很可能不是所有这些特征都与我们想要训练的模型的任务相关。此外，拥有太多特征（参见《Irrelevant Features and the Subset Selection Problem》）可能会导致模型性能不佳，因为较无益的特征会淹没那些更重要的特征。  
特征过多问题以维度灾难著称。随着特征数量的上升（数据维度增长），模型越来越难以学习特征与目标之间的映射关系。事实上，让模型表现良好所需的数据量与特征数量成指数关系。  
维度灾难与特征降维（也叫特征选择，去除不相关特征的过程）相对。这可以采用多种形式：主成分分析（PCA）、SelectKBest、使用模型中特征的重要性或使用深度神经网络进行自编码。但是，特征降维是另一篇文章的不同主题。到目前为止，我们知道我们可以使用特征工具以最小的努力从许多表中构造大量的特征！  
## 3.1.4 结论 
与机器学习中的许多主题一样，使用特征工具进行特征工程自动化是一个基于简单想法的复杂概念。使用实体集、实体和关联的概念，特征工具可以执行深度特征合成操作来构造新的特征。深度特征合成可以依次叠加特征基元：「聚合」，它们在多张表间的一对多关联中起作用，以及「转换」，是应用于单张表中一或多列以从多张表中构造新的特征的函数。

# 3.2 特征离散化  
数值离散化在数据挖掘和发现知识方面扮演者重要的角色。许多研究表明归纳任务能从离散化中获益：有离散值的规则通常是更简短，更容易理解，并且离散化能改善预测精度。文献中提到的很多归纳算法都需要离散特征。所有的这些特点促进研究人员和实践人员在开展机器学习或者数据挖掘任务之前都要进行连续特征离散化。在文献中能查到很多离散化的方法。本文旨在对离散化方法做一个系统性的研究，追溯它们发展的历史，以及这些方法对分类的影响，速度和精度之间的权衡。本文也给出了不同方法间的对比实验，并对结果进行了分析。本文的贡献主要有：对现存的离散化方法进行概述总结，对现有离散化方法进行归类的层次化结构描述，为进一步发展铺路，对典型的离散化方法的简要讨论，大量的试验和分析，在不同的应用环境下选择离散化方法的原则。同时也发现了一些需要去解决的问题，以及对离散化的进一步研究。
## 3.2.1 概述
数据通常以混合的形式出现：nominal，discrete，continuous。离散或者连续型数据属于定序数据类型，数据之间有次序，然而对于定类数据类型(nominal data types)，数据之间并不拥有次序。对于连续型属性其取值个数是无限大的，离散属性取值通常是有限的。这两种数据类型在学习分类树/规则时是不一样的。相比于连续属性值，离散属性值有一下优点：  
①通过离散化数据将被简化并且减少；  
② 离散特征更容易被理解，使用和解释；  
③ 离散化使学习更加准确，快速；  
④使用离散特征获得的结果(decision trees,induction rules)更加紧凑，简短，准确，结果更容易进行检查，比较，使用和重复使用；  
⑤ 很多分类学习算法只能处理离散数据。离散化是量化连续属性的过程。离散化的成功可以很大程度上扩展许多学习算法的使用领域。
本文回顾现存的离散化方法，标准化了离散化过程，以一个简要的框架总结它们，并且为研究和发展提供了一些方便的参考。本文余下的工作按照如下结构组织。Section 2总结了离散化方法的当前状态；Section 3以统一的词汇讨论了不同的离散化方法，定义了离散化的通用处理过程，考虑了不同的离散化结果评价方法，提出了新的用于离散化方法的层次框架，简洁的描述了典型的方法。当描述每一个典型的方法时，我们基于基准数据集给出了离散化结果。该数据集是一个小型数据集，通常用于离散化和分类工作，在这里使用是为了说明不同的算法的工作过程。Section 5比较了不同方法的实验结果，并对结果进行了分析。Section 6给出了选择离散化方法的直到准则和下一步工作。
## 3.2.2 当前状态
早期，诸如等宽(equal-width)，等频(equal-frequency)等技术用于离散化。伴随着精确度的需求以及有效的分类算法的发展，离散化的技术得到了快速发展。过去几年间，提出了许多离散化技术。试验表明离散化在减小数据大小、甚至改善预测精度方面都有着很大的潜力。离散化方法朝着不同的主线发展来满足不同的需求：监督与无监督；动态与静态；全局与局部；自顶向下与自底向上；直接与增量。  
监督与无监督：离散化是监督或者无监督，取决于离散化过程中离散化方法是否使用了分类信息。监督离散化使用了分类信息去指导离散化过程；然而无监督离散化却没有。没有分类信息的离散化方法类似于早期的等宽，等频技术：根据用户预先指定的宽度或者频率将连续的范围划分成各个子范围。  
对于连续属性值的分布不是均匀分布时，这可能不会给出好的结果。因此它对于异常值(outliers)是弱小的，因为他们明显影响了取值范围。为了克服这个缺点，引入了监督离散化方法，分类信息用于找到合适的用分割点定义的间隔。已经设计了不同的使用分类信息的方法去在连续属性上寻找有意义的间隔。如果没有分类信息可以利用，无监督离散化方法是唯一选择。没有太多的无监督离散化方法在文献中提到，可能归因于离散化通常与分类任务有关。  
动态与静态：离散化方法的应用可能是动态的，也有可能是静态的。所谓动态方法就是指离散化连续值是在分类器正在被建立的时候，诸如C4.5，静态离散化先于分类任务完成。
全局与局部：另一个分法是全局与局部。局部离散化方法是在实例空间的局部区域进行的离散化。然而全局离散化使用了全体实例空间进行离散化。当实例空间的一个区域被用来离散化时局部方法通常与动态离散化方法有关。  
自顶向下与自底向上：离散化方法也可以根据自顶向下与自底向上进行划分。自顶向下方法以一个空的分割点列表开始，在离散化进程中一直不断的往这个列表中通过添加新的分割点。自底向上以完整的列表开始，该列表用所有的连续特征值作为分割点，在离散化进程中一直不断的给这个列表中通过移除的分割点。  
直接与增量：直接方法同时划分K个间隔的范围(例如equal-width，equal-frequency，K-means)，需要一个额外的输入来决定间隔的个数。增量方法以一个简单的离散化开始，并伴随着改善或者提纯过程，需要停止准则来终止下一步的离散化。
综上所述，有许多离散化方法，并以不同的维度对它们进行分组。有时对用户而言对于手头的数据很难找到一个合适的方法。我们继续对这些关键问题进行全面的研究，包括：离散化过程的定义，性能度量，广泛的对比。  
本文的贡献主要有：   
1) 典型的离散化过程的简要描述；   
2) 新的层次化框架去分类文献中提到的现存的离散化方法；   
3) 对不同的离散化方法使用基准数据集对其结果进行系统论述；  
4) 对从这个框架下沿着基于公开数据集的分类学习算法的时间消耗和错    误率这两个维度选出的9个代表性的离散化方法进行对比；  
5) 对对比结果进行详细检查；  
6) 对不同的使用环境选用何种方法给出指导准则，给出下一步研究的方向；
## 3.2.3 离散化过程
首先我们阐明几个术语，以对典型的离散化过程进行简要描述。
### 3.2.3.1 术语和符号
**特征**  
特征，属性，变量是数据的一方面。通常在收集数据之前，特征应该被指定或选择。特征可能是离散的、连续的、定类的。本文我们的兴趣主要在离散连续特征的过程。接下来用M表示数据集中特征的个数。  
**实例**  
实例、元组、记录、数据点是一次数据记录所有特征的特征值。实例的集合产生数据集。通常数据集每行代表一个实例，每列代表一个特征。用N表示数据中实例的个数。
**分割点**
术语分割点指的是连续值范围中的一个实数值，它将这个范围分割成两个间隔。一个间隔是小于等于这个分割点的，另一个是大于分割点的。例如，一个连续的间隔[a,b]被分割成[a,c]和[c,b]，值c就是分割点。分割点也被叫做分离点。  
**元数**  
术语元数在离散化环境中就是间隔或者划分的个数。在离散化连续特征之前，元数被设置成k-连续特征的划分数。最大的分割点数是k-1。离散化过程减小了元数，但是在元数和分类或其他任务的性能之间做了折中。元数太大，会使学习过程变得更长，但是元数太小，会影响预测的精度。
### 3.2.3.2 典型离散化过程
使用“典型”，指的是单变量离散化。离散化可能是单变量的，也可能是多变量的。单变量离散化一次只离散化一个连续特征，然而多变量离散化同时考虑多个特征。本文主要考虑了单变量离散化，文章末尾简单讨论了多变量离散化，作为单变量离散化的扩展。 典型的离散化过程通常由以下4步组成：   
1) sort对离散化的特征的连续值进行排序；   
2) evaluate对为了合并的划分的或者邻近的间隔，评估分割点；  
3) split or merge通过一些准则，划分或者合并连续值的间隔；   
4) stop停止离散化。  
**排序**  
对特征的连续值进行降序或者升序排序都可以。如果不关心在离散化时进行排序，那么排序的计算代价将是非常昂贵的。对于所有的特征，排序在离散化开始之前完成，这将是一个全局处理，适用于将整个实例空间离散化。如果排序在过程的每一次迭代时完成，这将是一个局部处理，仅对整个实例空间的一个区域进行离散化。  
选择分割点 排序之后，离散化过程接下来的一步是寻找最好的分割点，对连续值的范围进行划分，或者寻找最好的一对邻近的间隔进行合并。一个典型的评价函数用来决定一次划分或者合并与类别标签的关系。文献中提到很多评价函数，诸如：熵测量、统计测量。关于评价函数与它们的应用将在下一小节进行讨论。  
**划分/合并**  
正如我们所知，对已自顶向下的方法，间隔是划分；然而对于自底向上的方法，间隔是合并。对于划分，需要评价分割点，选择最好的一个，并将连续值范围划分成两部分。分别对每一部分进行离散化直到满足停止准则。对于合并，评估邻近的间隔，找到最好的间隔进行合并。离散过程一直持续，间隔数随之减少，直到满足停止准则。  
**停止准则**    
停止准则是为了终止离散过程。它经常被元数和精度的折中所支配，因为这两个是正互相关的。我们可以将k作为离散化的结果的元数设置一个上界，事实上，上界k的设置是远小于N的，假设没有特征连续值的副本，停止准则可能是非常简单的，诸如在一开始固定间隔数，或者一个稍微复杂一些的如评价函数。在下一章节描述不同的停止准则。
### 3.2.3.3 离散化方法对比  
针对不同离散化方法获得的离散化数据，哪一个更好呢？看起来是一个简单的问题，却很难用简单的答案来回答。这是因为不同的方法之间的比较是一个复杂的问题，它依赖于用户在某一个特殊应用的需求。说复杂是因为不同方法的评价可以从多个方面来进行。我们列举了3个重要的维度：  
(1)总的间隔数，直观的，间隔点越少，离散化结果越好；但是强加于数据呈现是有限的。这导致第二个维度。  
(2)离散化引起的不一致性的个数。它不应该大于离散化之前原始数据的不一致数。如果最终结果是从数据训练一个分类器，应该考虑另一个观点。  
(3)预测精度-离散化对预测精度的改善。简言之，我们至少需要三个维度的考虑：简化(simplicity)、一致性(consistency)、精度(accuracy)。理想情况下，最好的离散化结果在这三方面应都有最高的得分。但事实上，它是不可能达到的，或者不需要的。为不同的离散化方法在这三方面提供一个均衡的观点也是本文的目标。
简化(simplicity)使用分割点的总数来定义的。精确度(accuracy)在交叉验证模式下运行分类器可以获得。一致性(consistency)是最小的不一致性计数值，可以通过以下3步计算而来(下面的描述中，模式pattern指没有类别标签的实例)：
如果两个实例有着相同的属性值，但是类别标签不一样，则这两个实例是不一致的；例如，如果两个实例是(0 1;0)和(0 1;1)，它们就是不一致的。
(2)一个模式的计数值是这个模式出现在数据中的次数减去最大的类别标签数；例如，假设有n个实例与这个模式匹配，其中label1的个数是c1, label2的个数是c2，label3的个数是c3，其中c1+c2+c3=n。如果c3是这三个中最大的，则该模式的不一致性计数值为(n-c3)。
(3)总的不一致性计数是一个特征集合中所有可能的模式的不一致性计数值之和。
## 3.2.4 离散化框架  
文献中提到很多离散化方法。正如上面所述的一样，根据不同的维度可以对这些方法进行分类。即：监督与无监督；动态与静态；全局与局部自顶向下与自底向上;直接与增量。可以根据这些维度的不同组合来对这些方法进行分组。我们希望构建一个系统的、可扩展的、能覆盖现在所有方法的框架。文献中提到的每一种离散化方法离散化一个特征：或者对连续值划分间隔；或者合并邻近的间隔。划分和合并根据是否使用分类信息可进一步分组成监督与无监督。 鉴于这些方面的考虑，我们提出了层次化框架，通过两种方法：划分和合并，描述了不同的离散化测量。接着我们考虑了方式是监督还是无监督。进一步我们将使用相同离散化度量方法的分组在一起，例如分箱和熵。我们将讨论如下表所示的基于划分和合并的分类方法的现存的不同的度量方式。  
在以下两个小节，将选择典型的方法进行深入讨论。它们相关的或者派生的度量方法也会简要涉及。对每一个离散化度量方式，我们主要给出： (1). 度量方式的定义； (2). 在离散化方法中的应用； (3). 使用的停止准则； (4). 对Iris数据集的离散化结果：每一个属性的分割点；  
在小节的最后，出于对比目的，我们给出了所有离散化度量方式的结果列表：不一致性的个数、离散化的分割点。
### 3.2.4.1 划分方法
我们以用于划分离散化方法的通用算法开始。
在离散化过程中划分算法由4步组成，它们是： (1). 对特征值排序； (2). 搜索合适的分割点； (3). 根据分割点划分连续值的范围； (4). 当满足停止准则的时候停止离散化否则继续； 

### 3.2.4.2 合并方法
我们以采用合并或者自底向上方法的离散化通用算法开始。在离散化过程中合并算法由4个重要步骤组成，它们是： (1). 对值进行排序； (2). 找到最好的两个相邻间隔； (3). 把这一对合并成一个间隔； (4). 满足停止条件时终止。
### 3.2.4.3 讨论
我们已经回顾了在划分和合并归类情况下的典型离散化方法。大部分方法是基于划分方法的。我们使用Iris数据集作为一个例子展示了不同的离散化方法的结果。两种度量方式直觉上的关系是：分割点越多，不一致计数越少。仔细观察表明让两个计数值都少可能有一个妥协的方法。因此，我们应该旨在去让两个数值都小。我们确实发现对于Iris数据一些方法比其他方法要好：Ent-MDLP是划分方法里边最好的；chi2是合并方法里边最好的。Chi2也表明，通过考虑一定程度的不一致性，在两种度量方式之间达到折中也是可能的。除离散化之外，Chi2也移除了具有一定程度的不一致性的特征。
在section2，我们通过5种不同的维度回顾了监督与无监督；动态与静态；全局与局部；自顶向下与自底向上；直接与增量的离散化方法。
