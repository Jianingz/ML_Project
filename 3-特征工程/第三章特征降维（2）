特征抽取是改变了原有的特征空间，并将其映射到了一个新的特征空间当中。
比如说，特征空间里包含城市名称、城市等级和城市人口。这三者是强相关的，但是又不能相互代替。因为北上广虽然同样都是一线城市，但是人口数量也还是有区别的。因此不能再特征选择时简单得将某一特征去掉。所以此时，为了达到降维的目的，我们可以将高维特征空间映射到一个低维的特征空间，但是在这个低维特征空间中，城市等级、城市名称、城市人口都能有所表达、有所体现。
这个过程就是特征抽取。其实降维的本质就是一个将特征从高维向低维空间映射的过程。抽取前后特征值的数值会发生变化，维度会降低，并且维度之间会更加独立。


其实，因为特征抽取是经过变换了的属性或者说是特征，是原来属性集的线性合并。特征抽取会获得一组更小、更精细的属性。用特征抽取建立起来的模型可能会获得更好的质量，因为数据能够被更少、更精细的属性所描述。而特征抽取的算法也有很多，比如说主成分分析法、因子分析法、非负矩阵因子分解NMP法等等算法。
这些算法在我们后面的学习和介绍中，也会逐一为大家详细讲解。

## 3.4	 特征降维原理及经典算法介绍

看电视的时候，我们发现电视中的每一个画面其实都包含着许许多多的像素点，那么实际上我们看到的每一个画面都是由成千上千万个维度构成的数据，但是我们在看电视的时候，并不会将这些所有信息都看进眼里，眼睛会自动将数据降维成为一个三维的数据，来供我们观看与理解，这也就是一个降维的过程。
其实， 降维方法可分为有监督降维方法、半监督降维方法、无监督降维方法。

有监督的降维方法有：线性判别式分析（Linear discriminant analysis, LDA）; 边缘Fisher分析（Marginal fisher analysis，MFA）;最大边缘准则（Maximum margin criterion, MMC）等。

无监督的降维方法有：主成分分析（Principal component analysis, PCA）, 近邻保持投影（Neighborhood preserving embedding, NPE)，局部保持投影（Locality preserving projections, LPP），稀疏保持投影（SPP）等。

半监督的降维的方法有：半监督降维（Semi-supervised dimensionality reduction, SSDR）, 半监督判别式分析（Semi-supervised discriminant analysis，SDA）等。

其中PCA是目前应用最为广泛的方法。

那么，特征降维到底有什么作用呢？
+  数据在低维下更容易处理、更容易使用，在进行特征比对的时候也更方便；
+  一些相关特征，特别是一些重要的、具有代表性的特征更能在数据中明确的显示出来；如果特征只有两维或者三维的话，更便于可视化展示；
+  能够去除数据噪声，维度太高将不可避免，有些特征会变成噪声。
+  降低算法开销，维数越低，所需要做的存储或是计算工作开销就会越小。

最后，我们在本章将对特征降维的几种经典算法进行讨论和分析。

### 3.4.1  主成分分析（principal component analysis,PCA）

主成分分析（Principal components analysis，以下简称PCA）是特征降维中一种最重要的方法之一。主成分分析法在数据噪音消除或是数据压缩消除冗余等诸多领域都有着极为广泛的应用。一般我们在讲到特征降维时，不得不提的一种有效特征降维算法就是主成分分析法PCA，那么下面，本章节我们就对PCA的原理做一个深入的探究和总结。

1.	 到底什么是主成分分析PCA？
主成分分析，其实我们从词义上也可以容易理解，主成分，也就是需要找出数据中最主要的特征、最主要的方面，并且试图用这个主成分、主特征来代替原始数据。具体来说，我们假设我们的原始数据集的数据个数是N，数据集的特征维度是N，那么此时我们试图找到一个n，使得n<N，并且这个n维的数据集能够尽可能的代表原始数据集的特征。但同时，毋庸置疑的是，数据从N维降到n维，信息肯定会有损失，因此我们希望这个损失能够尽可能的小。那么我们该怎样去衡量这个新的n维数据集能否尽可能代表原始数据集呢？

下面我们来从最简单的情况来探究，也就是当N=2,n=1，即将数据集从二维数据集降解到一维数据集。这个时候我们来看一张图。
（第一张图）
 
图中的点代表着原始数据集，而此刻，我们选取了两个一维特征向量来代表原始数据集。也就是图中的U1和U2，那么这两个特征哪个更好呢？

+ 首先，原始数据集中的每个数据点，到U1的距离，相对于原始数据集中的点到U2的距离，要更近一些，从这个角度来说，U1要能比U2更好的代表原始数据集。
+ 再者， 将原始数据集的点分别向U1、U2两个方向上投影后，我们发现，当原始数据集中的点向U1方向投影时能够更好得分散开来，而当向U2方向投影时，点与点的投影重合的很厉害，导致数据点与数据点无法区分。因此，从这个角度来说，U1也要比U2能更好得代表原始数据。

所以综上所述，我们可以判定，U1要比U2更好得代表原始数据。

从以上分析的这个简单示例来看，我们可以更直观的得到主成分分析PCA的两种等价推导：

+ 基于最小投影距离的计算
我们将距离从二维空间相应拓展到多维空间，即“找到一个超平面，使得样本点到这个超平面的距离足够近。”


+ 基于最大投影方差的计算
当我们将数据集从N维降到n维时，需要找到最大的n个特征值对应的特征向量。这n个特征向量组成的矩阵W即为我们需要的矩阵。对于原始数据集，我们只需要用z(i)=WTx(i)z(i)=WTx(i),就可以把原始数据集降维到最小投影距离的n维数据集。

2. PCA算法流程
从以上我们可以得出，求样本x(i)的n维主成分其实就是求样本集的协方差矩阵的前n个特征值对应特征向量矩阵W，然后对于每个样本x(i)x(i),做如下变换z(i)=WTx(i)，即达到降维的目的。

3.	核主成分分析KPCA介绍

在以上的PCA主成分分析法中，我们假设存在一个线性的超平面，能够让我们对原始数据集中的所有数据点进行投影。但是有些时候，我们的数据并非线性，那么就不能直接进行PCA降维。这里，就需要我们借用支持向量机一样的核函数的思想，我们可以先将数据集从N维映射到线性可分的高维M>N，然后再从M维降解到一个低维度n上，这时候n<N<M。从而达到最终的特征降维的效果。

另外，一般来讲，使用了核函数思想的主成分分析法一般被称作核主成分分析法，也就是Kernelized PCA, 以下简称KPCA。由于KPCA需要用到核函数的运算，因此，相对于主成分PCA来说，它的运算量要大很多。

4.	主成分分析法小结
通过我们以上对于主成分分析法PCA的详细介绍，相信大家对主成分分析法都有了更加全面而细致的认识。在本章节的最后，我们对主成分分析法PCA做一个小结。其实，主成分分析法是一种非监督学习的特征降维方法，它只需要特征值分解，就可以达到对数据进行压缩、去噪的效果。主成分分析法PCA在实际生活中的应用也非常广泛。但是，算法再好，都不可避免会有一些问题，其实PCA主成分分析也是存在一些缺陷的。因此，在应用过程中，也出现了很多PCA主成分分析的变种。比如说我们以上提到的核函数主成分分析法，KPCA，就是一个经典的例子。希望大家在解决具体问题的时候，能够结合实际情况，具体分析，将主成分分析法做更多的改进，以解决实际问题。

+ PCA算法的主要优点有：

　　　　1）仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

　　　　2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。

　　　　3）计算方法简单，主要运算是特征值分解，易于实现。

+ PCA算法的主要缺点有：

　　　　1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。

　　　　2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。




### 3.4.2    LDA线性判别分析

LDA线性判别分析是另外一种经典的降维方法。下面我们就对LDA线性判别分析做一下解析。LDA在模式识别领域有非常广泛的应用。

LDA是一种基于监督学习的特征降维技术。它数据集中的每一个样本都是有类别输出的。这一点是与PCA不同的，PCA是一种无监督降维技术，是不考虑样本类别输出的。

LDA的思想可以概括为“投影后同类内方差最小，类间方差最大”。
下面我们对这句话来进行解析，这句话也就是，当我们高维度的数据集中的数据向低维度投影之后，我们希望同属于一个类别的数据投影点可以尽可能地接近，而不同类别的数据其类别中心之间的距离可以尽可能的远。


下面，我们来举个例子，使得大家可以得到更直观的感受。

 
我们有两类数据，在图上显示为蓝色数据和红色数据。此时我们的数据是二维的，现在我们想对之做降维，降解成一维数据。我们希望能够将这些数据投影到一维的一条直线上，使得在这条直线上，同一种类别的数据投影点可以挨得尽可能的接近，而不同类别数据的数据中心之间的距离可以尽可能的远。也就是，红色和蓝色类别的数据中心之间的距离可以尽可能的远。
那么我们通过图上的两种投影方式可以发现，第一种投影后的效果，显然不如第二种投影效果好。
对于右侧的图，可以看到同类数据投影点非常集中。而红色与蓝色的数据中心之间的距离也相对较远。因此，第二种投影要好于第一种。

同样，我们将这个结论拓展到多维空间的降维，也就是我们需要寻找一个投影平面，使得投影后同类内方差最小，类间方差最大。


LDA 和 PCA算法之间的比较： 
LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。
+ 首先我们看看相同点：

　　　　1）两者均可以对数据进行降维。

　　　　2）两者在降维时均使用了矩阵特征分解的思想。

　　　　3）两者都假设数据符合高斯分布。


+ 我们接着看看不同点：

　　　　1）LDA是有监督的降维方法，而PCA是无监督的降维方法

　　　　2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。

　　　　3）LDA除了可以用于降维，还可以用于分类。

　　　　4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。

这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优。


### 3.4.3  局部线性嵌入 （LLE）

Locally linear embedding（LLE）[1] 是一种非线性降维算法，它能够使降维后的数据较好地保持原有 流形结构 。LLE可以说是流形学习方法最经典的工作之一。很多后续的流形学习、降维方法都与LLE有密切联系。
Locally linear embedding（LLE）局部线性嵌入是一种非线性降维的算法，它能够使得降维之后，数据仍旧较好得保持原有的流形结构。
那么这里，什么是流形（manifold）呢？
其实流形（manifold）就是对一般的几何对象的总称。比如食物，有面包、面条、包子等等。流形就包括各种维数的曲线、曲面等等。和一般的降维分析一样，流形学习是把一组在高维空间中的流形数据在低维空间中重新表示。
对于流形学习来讲，我们假设所有的数据是均匀采样于一个高维欧式空间中的低维流形，那么流形学习也就是从相应的高维采样数据中恢复出低维流形结构，也就是上文中提到的，找到高维空间中的低维流形，并求出相应的嵌入映射，以实现维数降低或者数据的可视化。也就是从观测到的现象中去寻找事物的本质，找到高维数据同低维数据之间存在的内在规律。

首先我们需要认识到的是， LLE在有些情况下并不适应。当数据分布在整个封闭的球面时，那么LLE就不能够将数据映射到二维空间，也不能保持原有的数据流形结构。所以我们在使用LLE来处理数据进行降维操作之前，应当首先保证数据不是分布在闭合的球面或者是椭球面上的。
一个流形降维的过程就好比是，我们有一块卷起来的布，我们希望这块布展开到一个二维平面上时，这块展开的布也能够在局部保持有布结构的特征，这其实也就是将这块布展开或拉开的一个过程。
在局部保持布结构的特征的方法有很多种，不同的保持方法对应不同的流形算法。其中，等距映射算法是需要找到所有样本全局的最优解，但是这种方法当数据量很大、样本维度很高的时候，计算量非常大。而，LLE则通过放弃所有样本全局最优解的降维方法，知识找到局部最优解来进行降维。那么我们同时假设样本集在局部是满足线性关系的，这样进一步减少降维的计算量。
那么LLE算法的思想是什么呢？

LLE算法首先是假设数据在较小的局部区域是线性的，那么某一个数据就可以用它邻域中的几个样本线性表示出来。比如说，我们有一个样本，我们在它的原始高维邻域里用K-近邻的方法找到和它最近的三个样本，那么我们就可以用这三个样本将之线性表示出来。
那么在我们经过LLE降维之后，我们仍然希望，原始样本投影后的结果，同近邻的三个样本的投影，仍然能保持一定的线性关系。且投影前后线性关系的权重系数是尽量不变或者最小改变的。
从这个过程可以看出，线性关系只在样本的附近起到了作用，在离样本距离远的其他样本对局部的线性关系并无任何影响，因此将局部的思想引入到了LLE降维当中，极大地缩减了降维的计算量。



LLE算法认为每一个数据点都可以由其近邻点的线性加权组合构造得到。算法的主要步骤分为三步：
 (1)寻找每个样本点的k个近邻点；
（2）由每个 样本点的近邻点计算出该样本点的局部重建权值矩阵；
（3）由该样本点的局部重建权值矩阵和其近邻点计算出该样本点的输出值。

LLE算法虽然简单高效，但是在一定情况下，仍然存在一些不可避免的问题。比如说如果近邻数K大于低维的维度D的时候，那么我们的权重系数矩阵不是满秩的。为了解决像这样类似的许多问题，所以一些基于LLE的变种算法逐渐演变而生。比如尝试不考虑保持局部的线性关系，而是保持局部的Hessian矩阵的二次型的关系。再比如MLLE算法对搜索到的最近邻的权重进行了度量，我们一般都是找距离最近的k个最近邻就可以了，而MLLE在找距离最近的k个最近邻的同时要考虑近邻的分布权重，它希望找到的近邻的分布权重尽量在样本的各个方向，而不是集中在一侧。


另一个比较好的LLE的变种是Local tangent space alignment(LTSA)，它希望保持数据集局部的几何关系，在降维后希望局部的几何关系得以保持，同时利用了局部几何到整体性质过渡的技巧。

以上提到的算法都是在LLE的基础上，再寻求优化演变而生的方法。

+ LLE总结
LLE的应用非常广泛，它是一种实现极为简单的方式，广泛使用在图形或是图像的特征降维当中，但是首先我们应当明确的一点是，LLE对数据的流形分布特征有着严格的要求。这些数据不能是闭合的流形，不能是稀疏的数据集，同时也不能是分布不均匀的数据集等等。因此，不可避免的，LLE有许多缺陷、限制了他的使用。因此也不断演变出了多种变种LLE。

+ LLE算法的主要优点有：
1）可以学习任意维的局部线性的低维流形
2）算法归结为稀疏矩阵特征分解，计算复杂度相对较小，实现容易。

+ LLE算法的主要缺点有：
1）算法所学习的流形只能是不闭合的，且样本集是稠密均匀的。
2）算法对最近邻样本数的选择敏感，不同的最近邻数对最后的降维结果有很大影响

### 3.4.4  半监督降维


半监督降维是传统降维方法的有效结合，它既有监督降维方法的优势，可以利用数据标号，又可以保持无监督降维方法中保持数据的某种结构信息，比如说数据的全局方差，或是是局部结构等等特征的优势。，因此，半监督降维能够克服传统降维方法的一些缺点，对于我们的降维研究有着非常重大的意义和价值，并且，半监督降维逐步走向成熟，拥有着广阔的应用前景。那么根据监督信息的不同，半监督降维方法可以被大致分为三类：

(1) 基于类别标号的方法; 
(2) 基于成对约束的方法;
(3) 基于其他监督信息的方法.

首先，我们来看基于类别标号的方法。

1.	基于类别标号的半监督降维

我们给出基于类别标号半监督降维方法的数学描述.假设有 N 个数据 1 {} N iX x == ,每个数据的维数为 D,即 xi∈RD(i=1,2,…,N).在所有数据中,L 个数据已经知道类别标号,记为 11 {( , )}L iii X x y == ,其中,xi 表示第i 个数据, yi是xi的类别标号,总共有C个类;剩下的数据没有类别标号,记为 21 {} N jjL Xx = += .半监督降维的目的是,利用有类 别标号的数据和无类别标号的数据 X={X1,X2},寻找数据的低维表示 Z={zi}∈Rd(d<D).

2.	基于成对约束的半监督降维


在半监督学习当中，我们除了可以利用类别标号这些先验信息来进行特征降维，同时，我们也可以利用其他形式的先验信息，比如说成对约束。在很多情况下，也许人们并不知道样本属于哪一个具体的类别标号，但是却知道两个样本是同属于一个类别的，或者是不属于一个类别。那么我们将这种先验监督信息称为成对约束。成对约束往往可以分为两种：正约束和负约束。正约束指的是两个样本同属于同一种类别，而负约束指的是两个样本并不属于同一类别。

半监督降维是基于有类标签的样本信息，找到高维输入数据的低维结构，同时保持原始高维数据和成对约束的结构不变，也就是在高维空间中满足正约束的样本在低维空间中相距很近，在高维空间中满足负约束的样本在低维空间中距离很远。

3.	基于其他监督学习的降维方法

半监督降维的方法除了可以利用类别标号的先验知识和成对约束先验知识作为监督信息之外，其实还存在着许多其他形式的监督信息。我们叫做基于其他监督信息的降维方法。

半监督降维方法目前仍存在的问题：

虽然半监督降维方法在大多数的应用场景中都能展现出极大的优势，表现出良好的性能，但是其实半监督降维方法也存在着一些自身的瓶颈与问题尚未解决：
+ 数据内在维数的估计：高维数据的内在维数往往并不是很高，如果能准确估计得到数据的内在维数，许多半监督降维方法的目标维数就不需要再人为地进行指定了。但是，当数据十分复杂或者是呈现出非线性关系的时候，估计数据的内在维数往往是一个非常困难的问题。

+ 监督信息的选择： 在我们实际的应用场景当中，获取到先验知识、监督信息往往需要付出极大的代价。因此，如何选择最具有信息量的监督信息这时候就显得极为重要。比如说分类，直观上来讲，位于类与类之间的数据对判别来说更有信息含量。如果能够有效选择这一部分数据并对其进行数据标注的话，将会在极大程度上提升半监督降维算法的性能。

+ 正约束和负约束的选择：对于基于成对约束的半监督降维方法，选择不同的正约束和负约束的比列，将会影响到降维的性能。目前已经有研究学者认为，正约束可能比负约束中含有更多的有用信息，但是仍然缺乏专业的研究来提供理由支撑。

+ 评价指标：其实，我们可以选择分类精度作为降维算法的评价指标。但是，降维的目的并不仅仅只是分类任何，还可以有很多其他形式的应用，比如说聚类和数据的可视化分析。所以对于我们的研究学习来讲，选择一个适当的评价指标来评价降维算法的性能，是正确评价算法性能的一个关键性因素。


## 3.5	 特征降维的应用


降维算法的本质将高维数据在低维空间进行有效表示，用来揭示数据的本质低维结构的。

如今，随着科学的不断进步和时代的不断发展，降维算法也开始在越来越多的领域有着极为广泛的应有。比如说，在神经科学之中，是用少数潜在或者隐藏的变量来概括大量神经元的活动。就比如说，当我们在头脑中反复思考的时候，或者在头脑中解决一些生活中难题的时候，所有活动都是在我们脑袋内部运行的，而并不在外部世界。通过这些潜在变量，就可以追踪一个人的思路。它提供了一种思路，让人们能理解大脑是如何区分不同的气味，在不确定的情况下做出决策，在没有真实动作时考虑要移动哪个肢体。所以现在降维算法可以应用在神经科学或是医学领域，为脑损伤和脑功能紊乱带来更好的治疗方法。

哥伦比亚大学统计学副教授约翰·康宁汉姆说：“科学研究的一个主要目标就是用简单的术语解释复杂的现象。传统的神经科学家一直在寻找简化单个神经元的方法，但他们发现，神经元的活动模式具有变化的特征，一次只检查一个神经元是很难理解这些活动的。降维算法为我们提供了一种涵盖单个神经元的变化的方法，为神经元之间的彼此互动找到简单的解释。”

另外，降维算法在图像分析、图像处理、数据处理领域则早就已经有着极为广泛的应用。而随着大数据时代的到来，人们面对繁冗复杂的数据，对大数据进行降维后的计算处理，将能够极大的减轻后续的工作量和计算量，降维算法必然会在大数据时代发挥出越来越重要的作用。大数据会变得更大，降维算法可能成为一种必不可少的数据处理方法。

## 3.6	本章总结与回顾

本章主要通过简单易懂的语言，极力寻找生活化的例子，将降维的概念传送给大家，希望大家能够对降维有更深刻、更具体的了解。而不只是停留在定义层面。
那么这章我们主要将降维是什么，以及降维的两种方法：特征抽取和特征选择介绍给了大家，并对特征抽取和特征选择的不同做了一定的阐释。
另外，我们介绍了经典的特征降维算法，本章重点在于特征降维算法可分为三大类。
降维方法可分为有监督降维方法、半监督降维方法、无监督降维方法。而每一类之下，又包括了许多不同的降维算法，我们各自选取了较为经典、应用广泛的算法进行了原理介绍及相关解读，试图让读者对这些不同的算法能够有更具体的认知。在这一部分，我们主要介绍了经典的PCA主成分分析法、LDA 线性判别分析降维算法、LLE局部线性嵌入降维算法以及半监督降维算法。
其中，半监督降维算法这一部分，是最新的、具有极为广阔应用场景的一类算法。半监督降维又可以分为三类，分别为：基于类别标号的半监督降维、基于成对约束的半监督降维、基于其他监督信息的半监督降维。这个部分需要大家进行着重消化。
在最后，我们对特征降维的应用前景做了阐述，我们发现，随着社会的发展、大数据时代的到来，特征降维技术将变得愈发重要，终将在科技舞台上发挥着不可替代的作用。

那么，以上就是本章节的全部内容，希望通过本章的学习，能够让大家对特征降维有一个更加全面而具体的认知，能够对特征降维的经典算法有更为深刻的理解，并且能够应用到具体的研究学习工作当中，最后，随着时代的发展，让我们一同见证，特征降维技术将为我们的科研所带来的极大便利吧！
