
## 第五章 模型评价方法

### 5.1 模型的评价方法介绍

5.1.1~5 accuracy，precision，recall，F1-score，ROC曲线

分别画图举例，要说出应用场景，例如什么情况用什么评价标准。

*   混淆矩阵
*   accuracy（准确率）
*   precision（精准度），recall（召回率），F1-score（宏观，微观）
*   ROC曲线图

python代码实现，例子。

（有视频参考）

### 5.2 项目实例运用

---

#模型评价方法介绍
为了对模型的效果进行评估，需要好的评估方法，还需要衡量模型泛化能力的评价标准。评价指标是机器学习任务中非常重要的一环。不同的机器学习任务有着不同的评价指标，同时同一种机器学习任务也有着不同的评价指标，每个指标的着重点不一样。如分类（classification）、回归（regression）、排序（ranking）、聚类（clustering）、热门主题模型（topic modeling）、推荐（recommendation）等。并且很多指标可以对多种不同的机器学习模型进行评价，如精确率－召回率（precision-recall），可以用在分类、推荐、排序等中。像分类、回归、排序都是监督式机器学习。

不同的机器学习任务有着不同的性能评价指标。例如，在垃圾邮件检测系统中，这个系统本质上是一个二分类问题（区分垃圾邮件vs正常邮件），可以使用准确率（Accuracy）、对数损失函数（log-loss）、AUC等评价方法。在股票预测中，这其实是一个实数序列数据预测问题，可以使用平方根误差（root mean square error， RMSE）等指标；又如在搜索引擎中进行与查询相关的项目排序中，可以使用精确率－召回率（precision-recall）等等。
![](https://upload-images.jianshu.io/upload_images/5401649-210d12e31d3258ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

###5.1.1 准确率
这是分类任务常见的评价标准，定义如下：**准确率**又称查准率（Precision），为分类任务中分类正确的样本数在总样本数中所占比例，错误率为分类错误的样本数在总样本中所占比例。准确率的计算方法为：

![](https://upload-images.jianshu.io/upload_images/5401649-31681a77fe06743a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
简单来说，在这样的一个应用场景中，某个社团有男的和女的两类，分类器根据自己的判断，将社团中的人分为男女两类。准确率需要得到的是该分类器判断正确的人占总人数的比例。显然，我们可以得到：假设分类器对100人的性别进行判断，而其中60人的性别判定正确，则该分类器的准确率就是60 %（60 / 100）。

根据准确率这一评价方法，我们可以在一些场景中得到一个分类器是否有效，但它并不总是能有效的评价一个分类器的工作。在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用Accuracy，即使全部预测成负类（不点击）Accuracy可以达到99% 以上，这样可以完爆其它很多分类器辛辛苦苦算的值，但是这个算法显然不是被需求所期待的，那怎么解决呢？这就是precision、recall和f1-score的出现的原因了。
###5.1.2 精度
准确率和错误率虽然常见，但是不一定能很好的满足需求。**精度（presicion）**又可以称为查准率。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率

对于一个二分类问题，可以将训练集的真实类别与模型预测得到的类别组合，得到以下四种类型：TP（True Positive），TN（True Nagetive），FP（False Positive），FN（False Nagetive）。所有的训练集中的样例都可以被分为这四种类型，组成一个混淆矩阵。

混淆矩阵（Confusion Matrix）是一个很重要的概念。混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。混淆矩阵是一个N X N矩阵，N为分类的个数。假如我们面对的是一个二分类问题，也就是N＝2，我们就得到一个2 X 2矩阵。

| 真实类别/预测类别 |   正例    |  负例          |
| :------:      | :---------:  | :------------:  |
| 正例        |   TP   |   FP  |
| 负例        |    FN    |  FN  |

对于一个m分的标准分类问题来说，也可以定义如上表所示m×m的m分混淆矩阵和每一个类属的Recall、Precision、F-measure和Accuracy值。


###5.1.3 召回
**召回率**又称查全率（Recall），召回率是指分类器分类正确的正样本个数占所有的正样本个数的比例。举例来说，召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。
![](https://upload-images.jianshu.io/upload_images/5401649-050b7181900d2689.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

精度和召回往往是一对矛盾的变量。如果按照预测为正例的概率大小进行排序，按照顺序依次进行预测，得到精度和召回，可以绘制一条P-R曲线。P-R图可以直观地展示样本整体的精度和召回的情况。
###5.1.4 F1

为了综合考虑精度和召回，也常常用F1度量。**F1-score**基于精度和召回的调和平均来定义，它的值更接近于Precision与Recall中较小的值。其计算方法为：即： 

![](https://upload-images.jianshu.io/upload_images/5401649-da5965368bfdca12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
精确率和准确率都高的情况下，F1值也会高。P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们。

除了F1之外，还有一个更普遍的计算公式：
F- = (α^2+1) PR/(α^2)P + R)

公式里的α（α>0）是用于衡量查全率对查准率的相对重要性。当α=1时即为标准的F1(基于P、R的调和平均)；当α>1时，查全率具有更大的影响；当α<1时查准率具有更大影响。

其中α=2和α=0.5是除了F1之外，两个常用的F-measure：
（1）当α=2，则表示recall的影响要大于precision；
（2）当α=0.5，则表示precision的影响要大于recall.

F1值在实际应用中较常用。相比于P、R的算术平均和几何平均（G-mean），F1值更重视较小值（不平衡数据下的稀有类），这也说明F1对于衡量数据更有利。
###5.1.5 ROC
**ROC**（Receiver Operating Characteristic）与P-R曲线类似，按照预测结果对样例进行排序，按照顺序依次进行预测，计算TPR和FPR。

召回率TPR(True Positive Rate) =TP/(TP+FN) ＝ a / (a+b)
取伪率FPR(False Positive Rate) = FP/(FP+TN) = c / (c+d)

ROC空间将取伪率（FPR）定义为 X 轴，召回率（TPR）定义为 Y 轴。即可以绘制一条ROC曲线。这两个指标之间相互制约。通俗地来说，即在TPR随着FPR递增的情况下，谁增长得更快，快多少的问题。TPR增长得越快，曲线越往上屈，反映了模型的分类性能就越好。显而易见，最好的分类器便是FPR＝0%，TPR＝100%，但是一般在实践中一个分类器很难会有这么好的效果，即一般TPR不等于1，FPR不等于0的。当正负样本不平衡时，这种模型评价方式比起一般的精确度评价方式的好处尤其显著。 
![ROC曲线](https://upload-images.jianshu.io/upload_images/5401649-d869f3c35958ca9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

具体如何描绘ROC曲线，如在二分类中，我们需要设定一个阈值，大于阈值分类正类，否则分为负类。因此，我们可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中的一些点，连接这些点就形成ROC曲线。ROC曲线会经过(0,0)与(1,1)这两点，实际上这两点的连线形成的ROC代表一个随机分类器，一般情况下分类器的ROC曲线会在这条对角连线上方。 

实际上，通过有限实例产生的ROC曲线实际上是一个阶梯函数，该曲线近似于实例数量接近无限时对应的ROC曲线。

对模型进行评价时，曲线距离左上角越近,证明分类器效果越好。当一条ROC曲线沿着将负分类器点和正分类器点连接构成的对角线，则该分类器的预测效果与随机猜测的差不多。若某ROC曲线可以将另一条ROC曲线完全包裹，则可以说明效果要好于被包裹的ROC曲线，否则，若两条ROC曲线存在交叉，则很难评价哪一条曲线更优。ROC曲线应尽量偏离参考线。

ROC曲线是通过与参照线进行比较来判断模型的好坏，虽然很直观好用，但这只是一种直觉上的定性分析，当使用ROC曲线对分类器进行评价时，如果对多个分类器进行比较时，如果直接使用ROC曲线很难去比较，只能通过将ROC分别画出来，然后进行肉眼比较，那么这种方法是非常不便的，因此我们需要一种定量的指标去比较，这个指标便是AUC了，即ROC曲线下的面积，面积越大，分类器的效果越好，AUC的值介于0.5到1.0之间。

ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。

**AUC**（area under curve）即ROC曲线下的面积。（随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值 比 分类器输出该负样本为正的那个概率值 要大的可能性）

假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。

通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的performance。
当0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
当AUC=0.5，跟随机猜测效果差不多，模型没有预测价值。

ROC曲线和AUC的优势：不受类分布的影响，适合与评估、比较类分布不平衡的数据集。因此ROC曲线与AUC已被广泛用于医疗决策制定、模式识别和数据挖掘等领域。但是ROC和AUC仅适合于两类问题 ,对多类问题 ,无法直接应用。

基尼系数应大于60%，就算好模型。基尼系数经常用于分类问题，其可以直接从AUC中得到。其公式为：Gini ＝ 2*AUC － 1

###应用
评价方法在很多python包中都有已经写好的函数，以下是sklearn中对应的函数名称以及描述：
|指标	|描述	|Scikit-learn函数|
| :------:      | :---------:  | :------------:  |
|Precision	|精准度|	from sklearn.metrics import precision_score|
|Recall	|召回率|	from sklearn.metrics import recall_score|
|F1|	F1值|	from sklearn.metrics import f1_score|
|Confusion Matrix|	混淆矩阵|	from sklearn.metrics import confusion_matrix|
|ROC|	ROC曲线|	from sklearn.metrics import roc|
|AUC|	ROC曲线下的面积|	from sklearn.metrics import auc|






